{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Polyphone_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqi1XPnDFbWV"
      },
      "source": [
        "# Feature Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k33n2juFbpK",
        "tags": []
      },
      "source": [
        "# Contains routines for labels creation, features extraction and normalization\n",
        "import os\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "from sklearn import preprocessing\n",
        "import joblib\n",
        "from IPython import embed\n",
        "import matplotlib.pyplot as plot\n",
        "import librosa\n",
        "plot.switch_backend('agg')\n",
        "\n",
        "\n",
        "class FeatureClass:\n",
        "    def __init__(self, dataset_dir='', feat_label_dir='', dataset='mic', is_eval=False):\n",
        "\n",
        "        # Input directories\n",
        "        self._feat_label_dir = feat_label_dir\n",
        "        self._dataset_dir = dataset_dir\n",
        "        self._dataset_combination = '{}_{}'.format(dataset, 'eval' if is_eval else 'dev')\n",
        "        self._aud_dir = os.path.join(self._dataset_dir, self._dataset_combination)\n",
        "\n",
        "        self._desc_dir = None if is_eval else os.path.join(self._dataset_dir, 'metadata_dev')\n",
        "\n",
        "        # Output directories\n",
        "        self._label_dir = None\n",
        "        self._feat_dir = None\n",
        "        self._feat_dir_norm = None\n",
        "\n",
        "        # Local parameters\n",
        "        self._is_eval = is_eval\n",
        "\n",
        "        self._fs = 48000\n",
        "        self._hop_len_s = 0.02\n",
        "        self._hop_len = int(self._fs * self._hop_len_s)\n",
        "        self._frame_res = self._fs / float(self._hop_len)\n",
        "        self._nb_frames_1s = int(self._frame_res)\n",
        "\n",
        "        self._win_len = 2 * self._hop_len\n",
        "        self._nfft = self._next_greater_power_of_2(self._win_len)\n",
        "\n",
        "        self._dataset = dataset\n",
        "        self._eps = np.spacing(np.float(1e-16))\n",
        "        self._nb_channels = 4\n",
        "\n",
        "        self._unique_classes = dict()\n",
        "        self._unique_classes = \\\n",
        "            {\n",
        "                'clearthroat': 2,\n",
        "                'cough': 8,\n",
        "                'doorslam': 9,\n",
        "                'drawer': 1,\n",
        "                'keyboard': 6,\n",
        "                'keysDrop': 4,\n",
        "                'knock': 0,\n",
        "                'laughter': 10,\n",
        "                'pageturn': 7,\n",
        "                'phone': 3,\n",
        "                'speech': 5\n",
        "            }\n",
        "\n",
        "        self._doa_resolution = 10\n",
        "        self._azi_list = range(-180, 180, self._doa_resolution)\n",
        "        self._length = len(self._azi_list)\n",
        "        self._ele_list = range(-40, 50, self._doa_resolution)\n",
        "        self._height = len(self._ele_list)\n",
        "\n",
        "        self._audio_max_len_samples = 60 * self._fs  \n",
        "\n",
        "        # For regression task only\n",
        "        self._default_azi = 180\n",
        "        self._default_ele = 50\n",
        "\n",
        "        if self._default_azi in self._azi_list:\n",
        "            print('ERROR: chosen default_azi value {} should not exist in azi_list'.format(self._default_azi))\n",
        "            exit()\n",
        "        if self._default_ele in self._ele_list:\n",
        "            print('ERROR: chosen default_ele value {} should not exist in ele_list'.format(self._default_ele))\n",
        "            exit()\n",
        "\n",
        "        self._max_frames = int(np.ceil(self._audio_max_len_samples / float(self._hop_len)))\n",
        "\n",
        "    def _load_audio(self, audio_path):\n",
        "        fs, audio = wav.read(audio_path)\n",
        "        audio = audio[:, :self._nb_channels] / 32768.0 + self._eps\n",
        "        if audio.shape[0] < self._audio_max_len_samples:\n",
        "            zero_pad = np.zeros((self._audio_max_len_samples - audio.shape[0], audio.shape[1]))\n",
        "            audio = np.vstack((audio, zero_pad))\n",
        "        elif audio.shape[0] > self._audio_max_len_samples:\n",
        "            audio = audio[:self._audio_max_len_samples, :]\n",
        "        return audio, fs\n",
        "\n",
        "    # INPUT FEATURES\n",
        "    @staticmethod\n",
        "    def _next_greater_power_of_2(x):\n",
        "        return 2 ** (x - 1).bit_length()\n",
        "\n",
        "    def _spectrogram(self, audio_input):\n",
        "        _nb_ch = audio_input.shape[1]\n",
        "        nb_bins = self._nfft // 2\n",
        "        spectra = np.zeros((self._max_frames, nb_bins, _nb_ch), dtype=complex)\n",
        "        for ch_cnt in range(_nb_ch):\n",
        "            stft_ch = librosa.core.stft(audio_input[:, ch_cnt], n_fft=self._nfft, hop_length=self._hop_len,\n",
        "                                        win_length=self._win_len, window='hann')\n",
        "            spectra[:, :, ch_cnt] = stft_ch[1:, :self._max_frames].T\n",
        "        return spectra\n",
        "\n",
        "    def _extract_spectrogram_for_file(self, audio_filename):\n",
        "        audio_in, fs = self._load_audio(os.path.join(self._aud_dir, audio_filename))\n",
        "        audio_spec = self._spectrogram(audio_in)\n",
        "        # print('\\t{}'.format(audio_spec.shape))\n",
        "        np.save(os.path.join(self._feat_dir, '{}.npy'.format(audio_filename.split('.')[0])), audio_spec.reshape(self._max_frames, -1))\n",
        "\n",
        "    # OUTPUT LABELS\n",
        "    def read_desc_file(self, desc_filename, in_sec=False):\n",
        "        desc_file = {\n",
        "            'class': list(), 'start': list(), 'end': list(), 'ele': list(), 'azi': list()\n",
        "        }\n",
        "        fid = open(desc_filename, 'r')\n",
        "        next(fid)\n",
        "        for line in fid:\n",
        "            split_line = line.strip().split(',')\n",
        "            desc_file['class'].append(split_line[0])\n",
        "            # desc_file['class'].append(split_line[0].split('.')[0][:-3])\n",
        "            if in_sec:\n",
        "                # return onset-offset time in seconds\n",
        "                desc_file['start'].append(float(split_line[1]))\n",
        "                desc_file['end'].append(float(split_line[2]))\n",
        "            else:\n",
        "                # return onset-offset time in frames\n",
        "                desc_file['start'].append(int(np.floor(float(split_line[1])*self._frame_res)))\n",
        "                desc_file['end'].append(int(np.ceil(float(split_line[2])*self._frame_res)))\n",
        "            desc_file['ele'].append(int(split_line[3]))\n",
        "            desc_file['azi'].append(int(split_line[4]))\n",
        "        fid.close()\n",
        "        return desc_file\n",
        "\n",
        "    def get_list_index(self, azi, ele):\n",
        "        azi = (azi - self._azi_list[0]) // 10\n",
        "        ele = (ele - self._ele_list[0]) // 10\n",
        "        return azi * self._height + ele\n",
        "\n",
        "    def get_matrix_index(self, ind):\n",
        "        azi, ele = ind // self._height, ind % self._height\n",
        "        azi = (azi * 10 + self._azi_list[0])\n",
        "        ele = (ele * 10 + self._ele_list[0])\n",
        "        return azi, ele\n",
        "\n",
        "    def _get_doa_labels_regr(self, _desc_file):\n",
        "        azi_label = self._default_azi*np.ones((self._max_frames, len(self._unique_classes)))\n",
        "        ele_label = self._default_ele*np.ones((self._max_frames, len(self._unique_classes)))\n",
        "        for i, ele_ang in enumerate(_desc_file['ele']):\n",
        "            start_frame = _desc_file['start'][i]\n",
        "            end_frame = self._max_frames if _desc_file['end'][i] > self._max_frames else _desc_file['end'][i]\n",
        "            azi_ang = _desc_file['azi'][i]\n",
        "            class_ind = self._unique_classes[_desc_file['class'][i]]\n",
        "            if (azi_ang >= self._azi_list[0]) & (azi_ang <= self._azi_list[-1]) & \\\n",
        "                    (ele_ang >= self._ele_list[0]) & (ele_ang <= self._ele_list[-1]):\n",
        "                azi_label[start_frame:end_frame + 1, class_ind] = azi_ang\n",
        "                ele_label[start_frame:end_frame + 1, class_ind] = ele_ang\n",
        "            else:\n",
        "                print('bad_angle {} {}'.format(azi_ang, ele_ang))\n",
        "        doa_label_regr = np.concatenate((azi_label, ele_label), axis=1)\n",
        "        return doa_label_regr\n",
        "\n",
        "    def _get_se_labels(self, _desc_file):\n",
        "        se_label = np.zeros((self._max_frames, len(self._unique_classes)))\n",
        "        for i, se_class in enumerate(_desc_file['class']):\n",
        "            start_frame = _desc_file['start'][i]\n",
        "            end_frame = self._max_frames if _desc_file['end'][i] > self._max_frames else _desc_file['end'][i]\n",
        "            se_label[start_frame:end_frame + 1, self._unique_classes[se_class]] = 1\n",
        "        return se_label\n",
        "\n",
        "    def get_labels_for_file(self, _desc_file):\n",
        "\n",
        "        se_label = self._get_se_labels(_desc_file)\n",
        "        doa_label = self._get_doa_labels_regr(_desc_file)\n",
        "        label_mat = np.concatenate((se_label, doa_label), axis=1)\n",
        "        # print(label_mat.shape)\n",
        "        return label_mat\n",
        "\n",
        "    def get_clas_labels_for_file(self, _desc_file):\n",
        "\n",
        "        _labels = np.zeros((self._max_frames, len(self._unique_classes), len(self._azi_list) * len(self._ele_list)))\n",
        "        for _ind, _start_frame in enumerate(_desc_file['start']):\n",
        "            _tmp_class = self._unique_classes[_desc_file['class'][_ind]]\n",
        "            _tmp_azi = _desc_file['azi'][_ind]\n",
        "            _tmp_ele = _desc_file['ele'][_ind]\n",
        "            _tmp_end = self._max_frames if _desc_file['end'][_ind] > self._max_frames else _desc_file['end'][_ind]\n",
        "            _tmp_ind = self.get_list_index(_tmp_azi, _tmp_ele)\n",
        "            _labels[_start_frame:_tmp_end + 1, _tmp_class, _tmp_ind] = 1\n",
        "\n",
        "        return _labels\n",
        "\n",
        "    # ------------------------------- EXTRACT FEATURE AND PREPROCESS IT -------------------------------\n",
        "    def extract_all_feature(self):\n",
        "        # setting up folders\n",
        "        self._feat_dir = self.get_unnormalized_feat_dir()\n",
        "        create_folder(self._feat_dir)\n",
        "\n",
        "        # extraction starts\n",
        "        print('Extracting spectrogram:')\n",
        "        print('\\t\\taud_dir {}\\n\\t\\tdesc_dir {}\\n\\t\\tfeat_dir {}'.format(\n",
        "            self._aud_dir, self._desc_dir, self._feat_dir))\n",
        "\n",
        "        for file_cnt, file_name in enumerate(os.listdir(self._aud_dir)):\n",
        "            print('{}: {}'.format(file_cnt, file_name))\n",
        "            wav_filename = '{}.wav'.format(file_name.split('.')[0])\n",
        "            self._extract_spectrogram_for_file(wav_filename)\n",
        "\n",
        "    def preprocess_features(self):\n",
        "        # Setting up folders and filenames\n",
        "        self._feat_dir = self.get_unnormalized_feat_dir()\n",
        "        self._feat_dir_norm = self.get_normalized_feat_dir()\n",
        "        create_folder(self._feat_dir_norm)\n",
        "        normalized_features_wts_file = self.get_normalized_wts_file()\n",
        "        spec_scaler = None\n",
        "\n",
        "        # pre-processing starts\n",
        "        if self._is_eval:\n",
        "            spec_scaler = joblib.load(normalized_features_wts_file)\n",
        "            print('Normalized_features_wts_file: {}. Loaded.'.format(normalized_features_wts_file))\n",
        "\n",
        "        else:\n",
        "            print('Estimating weights for normalizing feature files:')\n",
        "            print('\\t\\tfeat_dir: {}'.format(self._feat_dir))\n",
        "\n",
        "            spec_scaler = preprocessing.StandardScaler()\n",
        "            for file_cnt, file_name in enumerate(os.listdir(self._feat_dir)):\n",
        "                print('{}: {}'.format(file_cnt, file_name))\n",
        "                feat_file = np.load(os.path.join(self._feat_dir, file_name))\n",
        "                spec_scaler.partial_fit(np.concatenate((np.abs(feat_file), np.angle(feat_file)), axis=1))\n",
        "                del feat_file\n",
        "            joblib.dump(\n",
        "                spec_scaler,\n",
        "                normalized_features_wts_file\n",
        "            )\n",
        "            print('Normalized_features_wts_file: {}. Saved.'.format(normalized_features_wts_file))\n",
        "\n",
        "        print('Normalizing feature files:')\n",
        "        print('\\t\\tfeat_dir_norm {}'.format(self._feat_dir_norm))\n",
        "        for file_cnt, file_name in enumerate(os.listdir(self._feat_dir)):\n",
        "            print('{}: {}'.format(file_cnt, file_name))\n",
        "            feat_file = np.load(os.path.join(self._feat_dir, file_name))\n",
        "            feat_file = spec_scaler.transform(np.concatenate((np.abs(feat_file), np.angle(feat_file)), axis=1))\n",
        "            np.save(\n",
        "                os.path.join(self._feat_dir_norm, file_name),\n",
        "                feat_file\n",
        "            )\n",
        "            del feat_file\n",
        "\n",
        "        print('normalized files written to {}'.format(self._feat_dir_norm))\n",
        "\n",
        "    # ------------------------------- EXTRACT LABELS AND PREPROCESS IT -------------------------------\n",
        "    def extract_all_labels(self):\n",
        "        self._label_dir = self.get_label_dir()\n",
        "\n",
        "        print('Extracting labels:')\n",
        "        print('\\t\\taud_dir {}\\n\\t\\tdesc_dir {}\\n\\t\\tlabel_dir {}'.format(\n",
        "            self._aud_dir, self._desc_dir, self._label_dir))\n",
        "        create_folder(self._label_dir)\n",
        "\n",
        "        for file_cnt, file_name in enumerate(os.listdir(self._desc_dir)):\n",
        "            print('{}: {}'.format(file_cnt, file_name))\n",
        "            wav_filename = '{}.wav'.format(file_name.split('.')[0])\n",
        "            desc_file = self.read_desc_file(os.path.join(self._desc_dir, file_name))\n",
        "            label_mat = self.get_labels_for_file(desc_file)\n",
        "            np.save(os.path.join(self._label_dir, '{}.npy'.format(wav_filename.split('.')[0])), label_mat)\n",
        "\n",
        "    # ------------------------------- Misc public functions -------------------------------\n",
        "    def get_classes(self):\n",
        "        return self._unique_classes\n",
        "\n",
        "    def get_normalized_feat_dir(self):\n",
        "        return os.path.join(\n",
        "            self._feat_label_dir,\n",
        "            '{}_norm'.format(self._dataset_combination)\n",
        "        )\n",
        "\n",
        "    def get_unnormalized_feat_dir(self):\n",
        "        return os.path.join(\n",
        "            self._feat_label_dir,\n",
        "            '{}'.format(self._dataset_combination)\n",
        "        )\n",
        "\n",
        "    def get_label_dir(self):\n",
        "        if self._is_eval:\n",
        "            return None\n",
        "        else:\n",
        "            return os.path.join(\n",
        "                self._feat_label_dir, '{}_label'.format(self._dataset_combination)\n",
        "            )\n",
        "\n",
        "    def get_normalized_wts_file(self):\n",
        "        return os.path.join(\n",
        "            self._feat_label_dir,\n",
        "            '{}_wts'.format(self._dataset)\n",
        "        )\n",
        "\n",
        "    def get_default_azi_ele_regr(self):\n",
        "        return self._default_azi, self._default_ele\n",
        "\n",
        "    def get_nb_channels(self):\n",
        "        return self._nb_channels\n",
        "\n",
        "    def nb_frames_1s(self):\n",
        "        return self._nb_frames_1s\n",
        "\n",
        "    def get_hop_len_sec(self):\n",
        "        return self._hop_len_s\n",
        "\n",
        "    def get_azi_ele_list(self):\n",
        "        return self._azi_list, self._ele_list\n",
        "\n",
        "    def get_nb_frames(self):\n",
        "        return self._max_frames\n",
        "    \n",
        "\n",
        "def create_folder(folder_name):\n",
        "    if not os.path.exists(folder_name):\n",
        "        print('{} folder does not exist, creating it.'.format(folder_name))\n",
        "        os.makedirs(folder_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2mhljxiXKvb",
        "tags": []
      },
      "source": [
        "# Batch Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "T72riWodWxBd",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "outputId": "f744750d-608d-421e-a528-33a426bca654"
      },
      "source": [
        "process_str = 'dev, eval'  \n",
        "\n",
        "dataset_name = 'mic'  \n",
        "dataset_dir = 'D:\\sap'   # Base folder containing the mic and metadata folders\n",
        "feat_label_dir = 'D:\\sap\\sap_feat'  # Directory to dump extracted features and labels\n",
        "\n",
        "\n",
        "if 'dev' in process_str:\n",
        "    # -------------- Extract features and labels for development set -----------------------------\n",
        "    dev_feat_cls = FeatureClass(dataset=dataset_name, dataset_dir=dataset_dir,\n",
        "                                                  feat_label_dir=feat_label_dir)\n",
        "\n",
        "    # Extract features and normalize them\n",
        "    dev_feat_cls.extract_all_feature()\n",
        "    dev_feat_cls.preprocess_features()\n",
        "\n",
        "    # # Extract labels in regression mode\n",
        "    dev_feat_cls.extract_all_labels()\n",
        "\n",
        "\n",
        "if 'eval' in process_str:\n",
        "    # -----------------------------Extract ONLY features for evaluation set-----------------------------\n",
        "    eval_feat_cls = FeatureClass(dataset=dataset_name, dataset_dir=dataset_dir,\n",
        "                                                   feat_label_dir=feat_label_dir, is_eval=True)\n",
        "\n",
        "    # Extract features and normalize them\n",
        "    eval_feat_cls.extract_all_feature()\n",
        "    eval_feat_cls.preprocess_features()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<ipython-input-5-f8170aedd1de>:49: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._eps = np.spacing(np.float(1e-16))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "D:\\sap\\sap_feat\\mic_dev folder does not exist, creating it.\n",
            "Extracting spectrogram:\n",
            "\t\taud_dir D:\\sap\\mic_dev\n",
            "\t\tdesc_dir D:\\sap\\metadata_dev\n",
            "\t\tfeat_dir D:\\sap\\sap_feat\\mic_dev\n",
            "0: split1_ir0_ov1_1.wav\n",
            "1: split1_ir0_ov1_10.wav\n",
            "2: split1_ir0_ov1_2.wav\n",
            "3: split1_ir0_ov1_3.wav\n",
            "4: split1_ir0_ov1_4.wav\n",
            "5: split1_ir0_ov1_5.wav\n",
            "6: split1_ir0_ov1_6.wav\n",
            "7: split1_ir0_ov1_7.wav\n",
            "8: split1_ir0_ov1_8.wav\n",
            "9: split1_ir0_ov1_9.wav\n",
            "10: split1_ir0_ov2_11.wav\n",
            "11: split1_ir0_ov2_12.wav\n",
            "12: split1_ir0_ov2_13.wav\n",
            "13: split1_ir0_ov2_14.wav\n",
            "14: split1_ir0_ov2_15.wav\n",
            "15: split1_ir0_ov2_16.wav\n",
            "16: split1_ir0_ov2_17.wav\n",
            "17: split1_ir0_ov2_18.wav\n",
            "18: split1_ir0_ov2_19.wav\n",
            "19: split1_ir0_ov2_20.wav\n",
            "20: split1_ir1_ov1_21.wav\n",
            "21: split1_ir1_ov1_22.wav\n",
            "22: split1_ir1_ov1_23.wav\n",
            "23: split1_ir1_ov1_24.wav\n",
            "24: split1_ir1_ov1_25.wav\n",
            "25: split1_ir1_ov1_26.wav\n",
            "26: split1_ir1_ov1_27.wav\n",
            "27: split1_ir1_ov1_28.wav\n",
            "28: split1_ir1_ov1_29.wav\n",
            "29: split1_ir1_ov1_30.wav\n",
            "30: split1_ir1_ov2_31.wav\n",
            "31: split1_ir1_ov2_32.wav\n",
            "32: split1_ir1_ov2_33.wav\n",
            "33: split1_ir1_ov2_34.wav\n",
            "34: split1_ir1_ov2_35.wav\n",
            "35: split1_ir1_ov2_36.wav\n",
            "36: split1_ir1_ov2_37.wav\n",
            "37: split1_ir1_ov2_38.wav\n",
            "38: split1_ir1_ov2_39.wav\n",
            "39: split1_ir1_ov2_40.wav\n",
            "40: split1_ir2_ov1_41.wav\n",
            "41: split1_ir2_ov1_42.wav\n",
            "42: split1_ir2_ov1_43.wav\n",
            "43: split1_ir2_ov1_44.wav\n",
            "44: split1_ir2_ov1_45.wav\n",
            "45: split1_ir2_ov1_46.wav\n",
            "46: split1_ir2_ov1_47.wav\n",
            "47: split1_ir2_ov1_48.wav\n",
            "48: split1_ir2_ov1_49.wav\n",
            "49: split1_ir2_ov1_50.wav\n",
            "50: split1_ir2_ov2_51.wav\n",
            "51: split1_ir2_ov2_52.wav\n",
            "52: split1_ir2_ov2_53.wav\n",
            "53: split1_ir2_ov2_54.wav\n",
            "54: split1_ir2_ov2_55.wav\n",
            "55: split1_ir2_ov2_56.wav\n",
            "56: split1_ir2_ov2_57.wav\n",
            "57: split1_ir2_ov2_58.wav\n",
            "58: split1_ir2_ov2_59.wav\n",
            "59: split1_ir2_ov2_60.wav\n",
            "60: split1_ir3_ov1_61.wav\n",
            "61: split1_ir3_ov1_62.wav\n",
            "62: split1_ir3_ov1_63.wav\n",
            "63: split1_ir3_ov1_64.wav\n",
            "64: split1_ir3_ov1_65.wav\n",
            "65: split1_ir3_ov1_66.wav\n",
            "66: split1_ir3_ov1_67.wav\n",
            "67: split1_ir3_ov1_68.wav\n",
            "68: split1_ir3_ov1_69.wav\n",
            "69: split1_ir3_ov1_70.wav\n",
            "70: split1_ir3_ov2_71.wav\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<ipython-input-5-f8170aedd1de>:94: WavFileWarning: Reached EOF prematurely; finished at 4385428 bytes, expected 23060972 bytes from header.\n",
            "  fs, audio = wav.read(audio_path)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "71: split1_ir3_ov2_72.wav\n",
            "72: split1_ir3_ov2_73.wav\n",
            "73: split1_ir3_ov2_74.wav\n",
            "74: split1_ir3_ov2_75.wav\n",
            "75: split1_ir3_ov2_76.wav\n",
            "76: split1_ir3_ov2_77.wav\n",
            "77: split1_ir3_ov2_78.wav\n",
            "78: split1_ir3_ov2_79.wav\n",
            "79: split1_ir3_ov2_80.wav\n",
            "80: split1_ir4_ov1_81.wav\n",
            "81: split1_ir4_ov1_82.wav\n",
            "82: split1_ir4_ov1_83.wav\n",
            "83: split1_ir4_ov1_84.wav\n",
            "84: split1_ir4_ov1_85.wav\n",
            "85: split1_ir4_ov1_86.wav\n",
            "86: split1_ir4_ov1_87.wav\n",
            "87: split1_ir4_ov1_88.wav\n",
            "88: split1_ir4_ov1_89.wav\n",
            "89: split1_ir4_ov1_90.wav\n",
            "90: split1_ir4_ov2_100.wav\n",
            "91: split1_ir4_ov2_91.wav\n",
            "92: split1_ir4_ov2_92.wav\n",
            "93: split1_ir4_ov2_93.wav\n",
            "94: split1_ir4_ov2_94.wav\n",
            "95: split1_ir4_ov2_95.wav\n",
            "96: split1_ir4_ov2_96.wav\n",
            "97: split1_ir4_ov2_97.wav\n",
            "98: split1_ir4_ov2_98.wav\n",
            "99: split1_ir4_ov2_99.wav\n",
            "100: split2_ir0_ov1_1.wav\n",
            "101: split2_ir0_ov1_10.wav\n",
            "102: split2_ir0_ov1_2.wav\n",
            "103: split2_ir0_ov1_3.wav\n",
            "104: split2_ir0_ov1_4.wav\n",
            "105: split2_ir0_ov1_5.wav\n",
            "106: split2_ir0_ov1_6.wav\n",
            "107: split2_ir0_ov1_7.wav\n",
            "108: split2_ir0_ov1_8.wav\n",
            "109: split2_ir0_ov1_9.wav\n",
            "110: split2_ir0_ov2_11.wav\n",
            "111: split2_ir0_ov2_12.wav\n",
            "112: split2_ir0_ov2_13.wav\n",
            "113: split2_ir0_ov2_14.wav\n",
            "114: split2_ir0_ov2_15.wav\n",
            "115: split2_ir0_ov2_16.wav\n",
            "116: split2_ir0_ov2_17.wav\n",
            "117: split2_ir0_ov2_18.wav\n",
            "118: split2_ir0_ov2_19.wav\n",
            "119: split2_ir0_ov2_20.wav\n",
            "120: split2_ir1_ov1_21.wav\n",
            "121: split2_ir1_ov1_22.wav\n",
            "122: split2_ir1_ov1_23.wav\n",
            "123: split2_ir1_ov1_24.wav\n",
            "124: split2_ir1_ov1_25.wav\n",
            "125: split2_ir1_ov1_26.wav\n",
            "126: split2_ir1_ov1_27.wav\n",
            "127: split2_ir1_ov1_28.wav\n",
            "128: split2_ir1_ov1_29.wav\n",
            "129: split2_ir1_ov1_30.wav\n",
            "130: split2_ir1_ov2_31.wav\n",
            "131: split2_ir1_ov2_32.wav\n",
            "132: split2_ir1_ov2_33.wav\n",
            "133: split2_ir1_ov2_34.wav\n",
            "134: split2_ir1_ov2_35.wav\n",
            "135: split2_ir1_ov2_36.wav\n",
            "136: split2_ir1_ov2_37.wav\n",
            "137: split2_ir1_ov2_38.wav\n",
            "138: split2_ir1_ov2_39.wav\n",
            "139: split2_ir1_ov2_40.wav\n",
            "140: split2_ir2_ov1_41.wav\n",
            "141: split2_ir2_ov1_42.wav\n",
            "142: split2_ir2_ov1_43.wav\n",
            "143: split2_ir2_ov1_44.wav\n",
            "144: split2_ir2_ov1_45.wav\n",
            "145: split2_ir2_ov1_46.wav\n",
            "146: split2_ir2_ov1_47.wav\n",
            "147: split2_ir2_ov1_48.wav\n",
            "148: split2_ir2_ov1_49.wav\n",
            "149: split2_ir2_ov1_50.wav\n",
            "150: split2_ir2_ov2_51.wav\n",
            "151: split2_ir2_ov2_52.wav\n",
            "152: split2_ir2_ov2_53.wav\n",
            "153: split2_ir2_ov2_54.wav\n",
            "154: split2_ir2_ov2_55.wav\n",
            "155: split2_ir2_ov2_56.wav\n",
            "156: split2_ir2_ov2_57.wav\n",
            "157: split2_ir2_ov2_58.wav\n",
            "158: split2_ir2_ov2_59.wav\n",
            "159: split2_ir2_ov2_60.wav\n",
            "160: split2_ir3_ov1_61.wav\n",
            "161: split2_ir3_ov1_62.wav\n",
            "162: split2_ir3_ov1_63.wav\n",
            "163: split2_ir3_ov1_64.wav\n",
            "164: split2_ir3_ov1_65.wav\n",
            "165: split2_ir3_ov1_66.wav\n",
            "166: split2_ir3_ov1_67.wav\n",
            "167: split2_ir3_ov1_68.wav\n",
            "168: split2_ir3_ov1_69.wav\n",
            "169: split2_ir3_ov1_70.wav\n",
            "170: split2_ir3_ov2_71.wav\n",
            "171: split2_ir3_ov2_72.wav\n",
            "172: split2_ir3_ov2_73.wav\n",
            "173: split2_ir3_ov2_74.wav\n",
            "174: split2_ir3_ov2_75.wav\n",
            "175: split2_ir3_ov2_76.wav\n",
            "176: split2_ir3_ov2_77.wav\n",
            "177: split2_ir3_ov2_78.wav\n",
            "178: split2_ir3_ov2_79.wav\n",
            "179: split2_ir3_ov2_80.wav\n",
            "180: split2_ir4_ov1_81.wav\n",
            "181: split2_ir4_ov1_82.wav\n",
            "182: split2_ir4_ov1_83.wav\n",
            "183: split2_ir4_ov1_84.wav\n",
            "184: split2_ir4_ov1_85.wav\n",
            "185: split2_ir4_ov1_86.wav\n",
            "186: split2_ir4_ov1_87.wav\n",
            "187: split2_ir4_ov1_88.wav\n",
            "188: split2_ir4_ov1_89.wav\n",
            "189: split2_ir4_ov1_90.wav\n",
            "190: split2_ir4_ov2_100.wav\n",
            "191: split2_ir4_ov2_91.wav\n",
            "192: split2_ir4_ov2_92.wav\n",
            "193: split2_ir4_ov2_93.wav\n",
            "194: split2_ir4_ov2_94.wav\n",
            "195: split2_ir4_ov2_95.wav\n",
            "196: split2_ir4_ov2_96.wav\n",
            "197: split2_ir4_ov2_97.wav\n",
            "198: split2_ir4_ov2_98.wav\n",
            "199: split2_ir4_ov2_99.wav\n",
            "200: split3_ir0_ov1_1.wav\n",
            "201: split3_ir0_ov1_10.wav\n",
            "202: split3_ir0_ov1_2.wav\n",
            "203: split3_ir0_ov1_3.wav\n",
            "204: split3_ir0_ov1_4.wav\n",
            "205: split3_ir0_ov1_5.wav\n",
            "206: split3_ir0_ov1_6.wav\n",
            "207: split3_ir0_ov1_7.wav\n",
            "208: split3_ir0_ov1_8.wav\n",
            "209: split3_ir0_ov1_9.wav\n",
            "210: split3_ir0_ov2_11.wav\n",
            "211: split3_ir0_ov2_12.wav\n",
            "212: split3_ir0_ov2_13.wav\n",
            "213: split3_ir0_ov2_14.wav\n",
            "214: split3_ir0_ov2_15.wav\n",
            "215: split3_ir0_ov2_16.wav\n",
            "216: split3_ir0_ov2_17.wav\n",
            "217: split3_ir0_ov2_18.wav\n",
            "218: split3_ir0_ov2_19.wav\n",
            "219: split3_ir0_ov2_20.wav\n",
            "220: split3_ir1_ov1_21.wav\n",
            "221: split3_ir1_ov1_22.wav\n",
            "222: split3_ir1_ov1_23.wav\n",
            "223: split3_ir1_ov1_24.wav\n",
            "224: split3_ir1_ov1_25.wav\n",
            "225: split3_ir1_ov1_26.wav\n",
            "226: split3_ir1_ov1_27.wav\n",
            "227: split3_ir1_ov1_28.wav\n",
            "228: split3_ir1_ov1_29.wav\n",
            "229: split3_ir1_ov1_30.wav\n",
            "230: split3_ir1_ov2_31.wav\n",
            "231: split3_ir1_ov2_32.wav\n",
            "232: split3_ir1_ov2_33.wav\n",
            "233: split3_ir1_ov2_34.wav\n",
            "234: split3_ir1_ov2_35.wav\n",
            "235: split3_ir1_ov2_36.wav\n",
            "236: split3_ir1_ov2_37.wav\n",
            "237: split3_ir1_ov2_38.wav\n",
            "238: split3_ir1_ov2_39.wav\n",
            "239: split3_ir1_ov2_40.wav\n",
            "240: split3_ir2_ov1_41.wav\n",
            "241: split3_ir2_ov1_42.wav\n",
            "242: split3_ir2_ov1_43.wav\n",
            "243: split3_ir2_ov1_44.wav\n",
            "244: split3_ir2_ov1_45.wav\n",
            "245: split3_ir2_ov1_46.wav\n",
            "246: split3_ir2_ov1_47.wav\n",
            "247: split3_ir2_ov1_48.wav\n",
            "248: split3_ir2_ov1_49.wav\n",
            "249: split3_ir2_ov1_50.wav\n",
            "250: split3_ir2_ov2_51.wav\n",
            "251: split3_ir2_ov2_52.wav\n",
            "252: split3_ir2_ov2_53.wav\n",
            "253: split3_ir2_ov2_54.wav\n",
            "254: split3_ir2_ov2_55.wav\n",
            "255: split3_ir2_ov2_56.wav\n",
            "256: split3_ir2_ov2_57.wav\n",
            "257: split3_ir2_ov2_58.wav\n",
            "258: split3_ir2_ov2_59.wav\n",
            "259: split3_ir2_ov2_60.wav\n",
            "260: split3_ir3_ov1_61.wav\n",
            "261: split3_ir3_ov1_62.wav\n",
            "262: split3_ir3_ov1_63.wav\n",
            "263: split3_ir3_ov1_64.wav\n",
            "264: split3_ir3_ov1_65.wav\n",
            "265: split3_ir3_ov1_66.wav\n",
            "266: split3_ir3_ov1_67.wav\n",
            "267: split3_ir3_ov1_68.wav\n",
            "268: split3_ir3_ov1_69.wav\n",
            "269: split3_ir3_ov1_70.wav\n",
            "270: split3_ir3_ov2_71.wav\n",
            "271: split3_ir3_ov2_72.wav\n",
            "272: split3_ir3_ov2_73.wav\n",
            "273: split3_ir3_ov2_74.wav\n",
            "274: split3_ir3_ov2_75.wav\n",
            "275: split3_ir3_ov2_76.wav\n",
            "276: split3_ir3_ov2_77.wav\n",
            "277: split3_ir3_ov2_78.wav\n",
            "278: split3_ir3_ov2_79.wav\n",
            "279: split3_ir3_ov2_80.wav\n",
            "280: split3_ir4_ov1_81.wav\n",
            "281: split3_ir4_ov1_82.wav\n",
            "282: split3_ir4_ov1_83.wav\n",
            "283: split3_ir4_ov1_84.wav\n",
            "284: split3_ir4_ov1_85.wav\n",
            "285: split3_ir4_ov1_86.wav\n",
            "286: split3_ir4_ov1_87.wav\n",
            "287: split3_ir4_ov1_88.wav\n",
            "288: split3_ir4_ov1_89.wav\n",
            "289: split3_ir4_ov1_90.wav\n",
            "290: split3_ir4_ov2_100.wav\n",
            "291: split3_ir4_ov2_91.wav\n",
            "292: split3_ir4_ov2_92.wav\n",
            "293: split3_ir4_ov2_93.wav\n",
            "294: split3_ir4_ov2_94.wav\n",
            "295: split3_ir4_ov2_95.wav\n",
            "296: split3_ir4_ov2_96.wav\n",
            "297: split3_ir4_ov2_97.wav\n",
            "298: split3_ir4_ov2_98.wav\n",
            "299: split3_ir4_ov2_99.wav\n",
            "300: split4_ir0_ov1_1.wav\n",
            "301: split4_ir0_ov1_10.wav\n",
            "302: split4_ir0_ov1_2.wav\n",
            "303: split4_ir0_ov1_3.wav\n",
            "304: split4_ir0_ov1_4.wav\n",
            "305: split4_ir0_ov1_5.wav\n",
            "306: split4_ir0_ov1_6.wav\n",
            "307: split4_ir0_ov1_7.wav\n",
            "308: split4_ir0_ov1_8.wav\n",
            "309: split4_ir0_ov1_9.wav\n",
            "310: split4_ir0_ov2_11.wav\n",
            "311: split4_ir0_ov2_12.wav\n",
            "312: split4_ir0_ov2_13.wav\n",
            "313: split4_ir0_ov2_14.wav\n",
            "314: split4_ir0_ov2_15.wav\n",
            "315: split4_ir0_ov2_16.wav\n",
            "316: split4_ir0_ov2_17.wav\n",
            "317: split4_ir0_ov2_18.wav\n",
            "318: split4_ir0_ov2_19.wav\n",
            "319: split4_ir0_ov2_20.wav\n",
            "320: split4_ir1_ov1_21.wav\n",
            "321: split4_ir1_ov1_22.wav\n",
            "322: split4_ir1_ov1_23.wav\n",
            "323: split4_ir1_ov1_24.wav\n",
            "324: split4_ir1_ov1_25.wav\n",
            "325: split4_ir1_ov1_26.wav\n",
            "326: split4_ir1_ov1_27.wav\n",
            "327: split4_ir1_ov1_28.wav\n",
            "328: split4_ir1_ov1_29.wav\n",
            "329: split4_ir1_ov1_30.wav\n",
            "330: split4_ir1_ov2_31.wav\n",
            "331: split4_ir1_ov2_32.wav\n",
            "332: split4_ir1_ov2_33.wav\n",
            "333: split4_ir1_ov2_34.wav\n",
            "334: split4_ir1_ov2_35.wav\n",
            "335: split4_ir1_ov2_36.wav\n",
            "336: split4_ir1_ov2_37.wav\n",
            "337: split4_ir1_ov2_38.wav\n",
            "338: split4_ir1_ov2_39.wav\n",
            "339: split4_ir1_ov2_40.wav\n",
            "340: split4_ir2_ov1_41.wav\n",
            "341: split4_ir2_ov1_42.wav\n",
            "342: split4_ir2_ov1_43.wav\n",
            "343: split4_ir2_ov1_44.wav\n",
            "344: split4_ir2_ov1_45.wav\n",
            "345: split4_ir2_ov1_46.wav\n",
            "346: split4_ir2_ov1_47.wav\n",
            "347: split4_ir2_ov1_48.wav\n",
            "348: split4_ir2_ov1_49.wav\n",
            "349: split4_ir2_ov1_50.wav\n",
            "350: split4_ir2_ov2_51.wav\n",
            "351: split4_ir2_ov2_52.wav\n",
            "352: split4_ir2_ov2_53.wav\n",
            "353: split4_ir2_ov2_54.wav\n",
            "354: split4_ir2_ov2_55.wav\n",
            "355: split4_ir2_ov2_56.wav\n",
            "356: split4_ir2_ov2_57.wav\n",
            "357: split4_ir2_ov2_58.wav\n",
            "358: split4_ir2_ov2_59.wav\n",
            "359: split4_ir2_ov2_60.wav\n",
            "360: split4_ir3_ov1_61.wav\n",
            "361: split4_ir3_ov1_62.wav\n",
            "362: split4_ir3_ov1_63.wav\n",
            "363: split4_ir3_ov1_64.wav\n",
            "364: split4_ir3_ov1_65.wav\n",
            "365: split4_ir3_ov1_66.wav\n",
            "366: split4_ir3_ov1_67.wav\n",
            "367: split4_ir3_ov1_68.wav\n",
            "368: split4_ir3_ov1_69.wav\n",
            "369: split4_ir3_ov1_70.wav\n",
            "370: split4_ir3_ov2_71.wav\n",
            "371: split4_ir3_ov2_72.wav\n",
            "372: split4_ir3_ov2_73.wav\n",
            "373: split4_ir3_ov2_74.wav\n",
            "374: split4_ir3_ov2_75.wav\n",
            "375: split4_ir3_ov2_76.wav\n",
            "376: split4_ir3_ov2_77.wav\n",
            "377: split4_ir3_ov2_78.wav\n",
            "378: split4_ir3_ov2_79.wav\n",
            "379: split4_ir3_ov2_80.wav\n",
            "380: split4_ir4_ov1_81.wav\n",
            "381: split4_ir4_ov1_82.wav\n",
            "382: split4_ir4_ov1_83.wav\n",
            "383: split4_ir4_ov1_84.wav\n",
            "384: split4_ir4_ov1_85.wav\n",
            "385: split4_ir4_ov1_86.wav\n",
            "386: split4_ir4_ov1_87.wav\n",
            "387: split4_ir4_ov1_88.wav\n",
            "388: split4_ir4_ov1_89.wav\n",
            "389: split4_ir4_ov1_90.wav\n",
            "390: split4_ir4_ov2_100.wav\n",
            "391: split4_ir4_ov2_91.wav\n",
            "392: split4_ir4_ov2_92.wav\n",
            "393: split4_ir4_ov2_93.wav\n",
            "394: split4_ir4_ov2_94.wav\n",
            "395: split4_ir4_ov2_95.wav\n",
            "396: split4_ir4_ov2_96.wav\n",
            "397: split4_ir4_ov2_97.wav\n",
            "398: split4_ir4_ov2_98.wav\n",
            "399: split4_ir4_ov2_99.wav\n",
            "D:\\sap\\sap_feat\\mic_dev_norm folder does not exist, creating it.\n",
            "Estimating weights for normalizing feature files:\n",
            "\t\tfeat_dir: D:\\sap\\sap_feat\\mic_dev\n",
            "0: split1_ir0_ov1_1.npy\n",
            "1: split1_ir0_ov1_10.npy\n",
            "2: split1_ir0_ov1_2.npy\n",
            "3: split1_ir0_ov1_3.npy\n",
            "4: split1_ir0_ov1_4.npy\n",
            "5: split1_ir0_ov1_5.npy\n",
            "6: split1_ir0_ov1_6.npy\n",
            "7: split1_ir0_ov1_7.npy\n",
            "8: split1_ir0_ov1_8.npy\n",
            "9: split1_ir0_ov1_9.npy\n",
            "10: split1_ir0_ov2_11.npy\n",
            "11: split1_ir0_ov2_12.npy\n",
            "12: split1_ir0_ov2_13.npy\n",
            "13: split1_ir0_ov2_14.npy\n",
            "14: split1_ir0_ov2_15.npy\n",
            "15: split1_ir0_ov2_16.npy\n",
            "16: split1_ir0_ov2_17.npy\n",
            "17: split1_ir0_ov2_18.npy\n",
            "18: split1_ir0_ov2_19.npy\n",
            "19: split1_ir0_ov2_20.npy\n",
            "20: split1_ir1_ov1_21.npy\n",
            "21: split1_ir1_ov1_22.npy\n",
            "22: split1_ir1_ov1_23.npy\n",
            "23: split1_ir1_ov1_24.npy\n",
            "24: split1_ir1_ov1_25.npy\n",
            "25: split1_ir1_ov1_26.npy\n",
            "26: split1_ir1_ov1_27.npy\n",
            "27: split1_ir1_ov1_28.npy\n",
            "28: split1_ir1_ov1_29.npy\n",
            "29: split1_ir1_ov1_30.npy\n",
            "30: split1_ir1_ov2_31.npy\n",
            "31: split1_ir1_ov2_32.npy\n",
            "32: split1_ir1_ov2_33.npy\n",
            "33: split1_ir1_ov2_34.npy\n",
            "34: split1_ir1_ov2_35.npy\n",
            "35: split1_ir1_ov2_36.npy\n",
            "36: split1_ir1_ov2_37.npy\n",
            "37: split1_ir1_ov2_38.npy\n",
            "38: split1_ir1_ov2_39.npy\n",
            "39: split1_ir1_ov2_40.npy\n",
            "40: split1_ir2_ov1_41.npy\n",
            "41: split1_ir2_ov1_42.npy\n",
            "42: split1_ir2_ov1_43.npy\n",
            "43: split1_ir2_ov1_44.npy\n",
            "44: split1_ir2_ov1_45.npy\n",
            "45: split1_ir2_ov1_46.npy\n",
            "46: split1_ir2_ov1_47.npy\n",
            "47: split1_ir2_ov1_48.npy\n",
            "48: split1_ir2_ov1_49.npy\n",
            "49: split1_ir2_ov1_50.npy\n",
            "50: split1_ir2_ov2_51.npy\n",
            "51: split1_ir2_ov2_52.npy\n",
            "52: split1_ir2_ov2_53.npy\n",
            "53: split1_ir2_ov2_54.npy\n",
            "54: split1_ir2_ov2_55.npy\n",
            "55: split1_ir2_ov2_56.npy\n",
            "56: split1_ir2_ov2_57.npy\n",
            "57: split1_ir2_ov2_58.npy\n",
            "58: split1_ir2_ov2_59.npy\n",
            "59: split1_ir2_ov2_60.npy\n",
            "60: split1_ir3_ov1_61.npy\n",
            "61: split1_ir3_ov1_62.npy\n",
            "62: split1_ir3_ov1_63.npy\n",
            "63: split1_ir3_ov1_64.npy\n",
            "64: split1_ir3_ov1_65.npy\n",
            "65: split1_ir3_ov1_66.npy\n",
            "66: split1_ir3_ov1_67.npy\n",
            "67: split1_ir3_ov1_68.npy\n",
            "68: split1_ir3_ov1_69.npy\n",
            "69: split1_ir3_ov1_70.npy\n",
            "70: split1_ir3_ov2_71.npy\n",
            "71: split1_ir3_ov2_72.npy\n",
            "72: split1_ir3_ov2_73.npy\n",
            "73: split1_ir3_ov2_74.npy\n",
            "74: split1_ir3_ov2_75.npy\n",
            "75: split1_ir3_ov2_76.npy\n",
            "76: split1_ir3_ov2_77.npy\n",
            "77: split1_ir3_ov2_78.npy\n",
            "78: split1_ir3_ov2_79.npy\n",
            "79: split1_ir3_ov2_80.npy\n",
            "80: split1_ir4_ov1_81.npy\n",
            "81: split1_ir4_ov1_82.npy\n",
            "82: split1_ir4_ov1_83.npy\n",
            "83: split1_ir4_ov1_84.npy\n",
            "84: split1_ir4_ov1_85.npy\n",
            "85: split1_ir4_ov1_86.npy\n",
            "86: split1_ir4_ov1_87.npy\n",
            "87: split1_ir4_ov1_88.npy\n",
            "88: split1_ir4_ov1_89.npy\n",
            "89: split1_ir4_ov1_90.npy\n",
            "90: split1_ir4_ov2_100.npy\n",
            "91: split1_ir4_ov2_91.npy\n",
            "92: split1_ir4_ov2_92.npy\n",
            "93: split1_ir4_ov2_93.npy\n",
            "94: split1_ir4_ov2_94.npy\n",
            "95: split1_ir4_ov2_95.npy\n",
            "96: split1_ir4_ov2_96.npy\n",
            "97: split1_ir4_ov2_97.npy\n",
            "98: split1_ir4_ov2_98.npy\n",
            "99: split1_ir4_ov2_99.npy\n",
            "100: split2_ir0_ov1_1.npy\n",
            "101: split2_ir0_ov1_10.npy\n",
            "102: split2_ir0_ov1_2.npy\n",
            "103: split2_ir0_ov1_3.npy\n",
            "104: split2_ir0_ov1_4.npy\n",
            "105: split2_ir0_ov1_5.npy\n",
            "106: split2_ir0_ov1_6.npy\n",
            "107: split2_ir0_ov1_7.npy\n",
            "108: split2_ir0_ov1_8.npy\n",
            "109: split2_ir0_ov1_9.npy\n",
            "110: split2_ir0_ov2_11.npy\n",
            "111: split2_ir0_ov2_12.npy\n",
            "112: split2_ir0_ov2_13.npy\n",
            "113: split2_ir0_ov2_14.npy\n",
            "114: split2_ir0_ov2_15.npy\n",
            "115: split2_ir0_ov2_16.npy\n",
            "116: split2_ir0_ov2_17.npy\n",
            "117: split2_ir0_ov2_18.npy\n",
            "118: split2_ir0_ov2_19.npy\n",
            "119: split2_ir0_ov2_20.npy\n",
            "120: split2_ir1_ov1_21.npy\n",
            "121: split2_ir1_ov1_22.npy\n",
            "122: split2_ir1_ov1_23.npy\n",
            "123: split2_ir1_ov1_24.npy\n",
            "124: split2_ir1_ov1_25.npy\n",
            "125: split2_ir1_ov1_26.npy\n",
            "126: split2_ir1_ov1_27.npy\n",
            "127: split2_ir1_ov1_28.npy\n",
            "128: split2_ir1_ov1_29.npy\n",
            "129: split2_ir1_ov1_30.npy\n",
            "130: split2_ir1_ov2_31.npy\n",
            "131: split2_ir1_ov2_32.npy\n",
            "132: split2_ir1_ov2_33.npy\n",
            "133: split2_ir1_ov2_34.npy\n",
            "134: split2_ir1_ov2_35.npy\n",
            "135: split2_ir1_ov2_36.npy\n",
            "136: split2_ir1_ov2_37.npy\n",
            "137: split2_ir1_ov2_38.npy\n",
            "138: split2_ir1_ov2_39.npy\n",
            "139: split2_ir1_ov2_40.npy\n",
            "140: split2_ir2_ov1_41.npy\n",
            "141: split2_ir2_ov1_42.npy\n",
            "142: split2_ir2_ov1_43.npy\n",
            "143: split2_ir2_ov1_44.npy\n",
            "144: split2_ir2_ov1_45.npy\n",
            "145: split2_ir2_ov1_46.npy\n",
            "146: split2_ir2_ov1_47.npy\n",
            "147: split2_ir2_ov1_48.npy\n",
            "148: split2_ir2_ov1_49.npy\n",
            "149: split2_ir2_ov1_50.npy\n",
            "150: split2_ir2_ov2_51.npy\n",
            "151: split2_ir2_ov2_52.npy\n",
            "152: split2_ir2_ov2_53.npy\n",
            "153: split2_ir2_ov2_54.npy\n",
            "154: split2_ir2_ov2_55.npy\n",
            "155: split2_ir2_ov2_56.npy\n",
            "156: split2_ir2_ov2_57.npy\n",
            "157: split2_ir2_ov2_58.npy\n",
            "158: split2_ir2_ov2_59.npy\n",
            "159: split2_ir2_ov2_60.npy\n",
            "160: split2_ir3_ov1_61.npy\n",
            "161: split2_ir3_ov1_62.npy\n",
            "162: split2_ir3_ov1_63.npy\n",
            "163: split2_ir3_ov1_64.npy\n",
            "164: split2_ir3_ov1_65.npy\n",
            "165: split2_ir3_ov1_66.npy\n",
            "166: split2_ir3_ov1_67.npy\n",
            "167: split2_ir3_ov1_68.npy\n",
            "168: split2_ir3_ov1_69.npy\n",
            "169: split2_ir3_ov1_70.npy\n",
            "170: split2_ir3_ov2_71.npy\n",
            "171: split2_ir3_ov2_72.npy\n",
            "172: split2_ir3_ov2_73.npy\n",
            "173: split2_ir3_ov2_74.npy\n",
            "174: split2_ir3_ov2_75.npy\n",
            "175: split2_ir3_ov2_76.npy\n",
            "176: split2_ir3_ov2_77.npy\n",
            "177: split2_ir3_ov2_78.npy\n",
            "178: split2_ir3_ov2_79.npy\n",
            "179: split2_ir3_ov2_80.npy\n",
            "180: split2_ir4_ov1_81.npy\n",
            "181: split2_ir4_ov1_82.npy\n",
            "182: split2_ir4_ov1_83.npy\n",
            "183: split2_ir4_ov1_84.npy\n",
            "184: split2_ir4_ov1_85.npy\n",
            "185: split2_ir4_ov1_86.npy\n",
            "186: split2_ir4_ov1_87.npy\n",
            "187: split2_ir4_ov1_88.npy\n",
            "188: split2_ir4_ov1_89.npy\n",
            "189: split2_ir4_ov1_90.npy\n",
            "190: split2_ir4_ov2_100.npy\n",
            "191: split2_ir4_ov2_91.npy\n",
            "192: split2_ir4_ov2_92.npy\n",
            "193: split2_ir4_ov2_93.npy\n",
            "194: split2_ir4_ov2_94.npy\n",
            "195: split2_ir4_ov2_95.npy\n",
            "196: split2_ir4_ov2_96.npy\n",
            "197: split2_ir4_ov2_97.npy\n",
            "198: split2_ir4_ov2_98.npy\n",
            "199: split2_ir4_ov2_99.npy\n",
            "200: split3_ir0_ov1_1.npy\n",
            "201: split3_ir0_ov1_10.npy\n",
            "202: split3_ir0_ov1_2.npy\n",
            "203: split3_ir0_ov1_3.npy\n",
            "204: split3_ir0_ov1_4.npy\n",
            "205: split3_ir0_ov1_5.npy\n",
            "206: split3_ir0_ov1_6.npy\n",
            "207: split3_ir0_ov1_7.npy\n",
            "208: split3_ir0_ov1_8.npy\n",
            "209: split3_ir0_ov1_9.npy\n",
            "210: split3_ir0_ov2_11.npy\n",
            "211: split3_ir0_ov2_12.npy\n",
            "212: split3_ir0_ov2_13.npy\n",
            "213: split3_ir0_ov2_14.npy\n",
            "214: split3_ir0_ov2_15.npy\n",
            "215: split3_ir0_ov2_16.npy\n",
            "216: split3_ir0_ov2_17.npy\n",
            "217: split3_ir0_ov2_18.npy\n",
            "218: split3_ir0_ov2_19.npy\n",
            "219: split3_ir0_ov2_20.npy\n",
            "220: split3_ir1_ov1_21.npy\n",
            "221: split3_ir1_ov1_22.npy\n",
            "222: split3_ir1_ov1_23.npy\n",
            "223: split3_ir1_ov1_24.npy\n",
            "224: split3_ir1_ov1_25.npy\n",
            "225: split3_ir1_ov1_26.npy\n",
            "226: split3_ir1_ov1_27.npy\n",
            "227: split3_ir1_ov1_28.npy\n",
            "228: split3_ir1_ov1_29.npy\n",
            "229: split3_ir1_ov1_30.npy\n",
            "230: split3_ir1_ov2_31.npy\n",
            "231: split3_ir1_ov2_32.npy\n",
            "232: split3_ir1_ov2_33.npy\n",
            "233: split3_ir1_ov2_34.npy\n",
            "234: split3_ir1_ov2_35.npy\n",
            "235: split3_ir1_ov2_36.npy\n",
            "236: split3_ir1_ov2_37.npy\n",
            "237: split3_ir1_ov2_38.npy\n",
            "238: split3_ir1_ov2_39.npy\n",
            "239: split3_ir1_ov2_40.npy\n",
            "240: split3_ir2_ov1_41.npy\n",
            "241: split3_ir2_ov1_42.npy\n",
            "242: split3_ir2_ov1_43.npy\n",
            "243: split3_ir2_ov1_44.npy\n",
            "244: split3_ir2_ov1_45.npy\n",
            "245: split3_ir2_ov1_46.npy\n",
            "246: split3_ir2_ov1_47.npy\n",
            "247: split3_ir2_ov1_48.npy\n",
            "248: split3_ir2_ov1_49.npy\n",
            "249: split3_ir2_ov1_50.npy\n",
            "250: split3_ir2_ov2_51.npy\n",
            "251: split3_ir2_ov2_52.npy\n",
            "252: split3_ir2_ov2_53.npy\n",
            "253: split3_ir2_ov2_54.npy\n",
            "254: split3_ir2_ov2_55.npy\n",
            "255: split3_ir2_ov2_56.npy\n",
            "256: split3_ir2_ov2_57.npy\n",
            "257: split3_ir2_ov2_58.npy\n",
            "258: split3_ir2_ov2_59.npy\n",
            "259: split3_ir2_ov2_60.npy\n",
            "260: split3_ir3_ov1_61.npy\n",
            "261: split3_ir3_ov1_62.npy\n",
            "262: split3_ir3_ov1_63.npy\n",
            "263: split3_ir3_ov1_64.npy\n",
            "264: split3_ir3_ov1_65.npy\n",
            "265: split3_ir3_ov1_66.npy\n",
            "266: split3_ir3_ov1_67.npy\n",
            "267: split3_ir3_ov1_68.npy\n",
            "268: split3_ir3_ov1_69.npy\n",
            "269: split3_ir3_ov1_70.npy\n",
            "270: split3_ir3_ov2_71.npy\n",
            "271: split3_ir3_ov2_72.npy\n",
            "272: split3_ir3_ov2_73.npy\n",
            "273: split3_ir3_ov2_74.npy\n",
            "274: split3_ir3_ov2_75.npy\n",
            "275: split3_ir3_ov2_76.npy\n",
            "276: split3_ir3_ov2_77.npy\n",
            "277: split3_ir3_ov2_78.npy\n",
            "278: split3_ir3_ov2_79.npy\n",
            "279: split3_ir3_ov2_80.npy\n",
            "280: split3_ir4_ov1_81.npy\n",
            "281: split3_ir4_ov1_82.npy\n",
            "282: split3_ir4_ov1_83.npy\n",
            "283: split3_ir4_ov1_84.npy\n",
            "284: split3_ir4_ov1_85.npy\n",
            "285: split3_ir4_ov1_86.npy\n",
            "286: split3_ir4_ov1_87.npy\n",
            "287: split3_ir4_ov1_88.npy\n",
            "288: split3_ir4_ov1_89.npy\n",
            "289: split3_ir4_ov1_90.npy\n",
            "290: split3_ir4_ov2_100.npy\n",
            "291: split3_ir4_ov2_91.npy\n",
            "292: split3_ir4_ov2_92.npy\n",
            "293: split3_ir4_ov2_93.npy\n",
            "294: split3_ir4_ov2_94.npy\n",
            "295: split3_ir4_ov2_95.npy\n",
            "296: split3_ir4_ov2_96.npy\n",
            "297: split3_ir4_ov2_97.npy\n",
            "298: split3_ir4_ov2_98.npy\n",
            "299: split3_ir4_ov2_99.npy\n",
            "300: split4_ir0_ov1_1.npy\n",
            "301: split4_ir0_ov1_10.npy\n",
            "302: split4_ir0_ov1_2.npy\n",
            "303: split4_ir0_ov1_3.npy\n",
            "304: split4_ir0_ov1_4.npy\n",
            "305: split4_ir0_ov1_5.npy\n",
            "306: split4_ir0_ov1_6.npy\n",
            "307: split4_ir0_ov1_7.npy\n",
            "308: split4_ir0_ov1_8.npy\n",
            "309: split4_ir0_ov1_9.npy\n",
            "310: split4_ir0_ov2_11.npy\n",
            "311: split4_ir0_ov2_12.npy\n",
            "312: split4_ir0_ov2_13.npy\n",
            "313: split4_ir0_ov2_14.npy\n",
            "314: split4_ir0_ov2_15.npy\n",
            "315: split4_ir0_ov2_16.npy\n",
            "316: split4_ir0_ov2_17.npy\n",
            "317: split4_ir0_ov2_18.npy\n",
            "318: split4_ir0_ov2_19.npy\n",
            "319: split4_ir0_ov2_20.npy\n",
            "320: split4_ir1_ov1_21.npy\n",
            "321: split4_ir1_ov1_22.npy\n",
            "322: split4_ir1_ov1_23.npy\n",
            "323: split4_ir1_ov1_24.npy\n",
            "324: split4_ir1_ov1_25.npy\n",
            "325: split4_ir1_ov1_26.npy\n",
            "326: split4_ir1_ov1_27.npy\n",
            "327: split4_ir1_ov1_28.npy\n",
            "328: split4_ir1_ov1_29.npy\n",
            "329: split4_ir1_ov1_30.npy\n",
            "330: split4_ir1_ov2_31.npy\n",
            "331: split4_ir1_ov2_32.npy\n",
            "332: split4_ir1_ov2_33.npy\n",
            "333: split4_ir1_ov2_34.npy\n",
            "334: split4_ir1_ov2_35.npy\n",
            "335: split4_ir1_ov2_36.npy\n",
            "336: split4_ir1_ov2_37.npy\n",
            "337: split4_ir1_ov2_38.npy\n",
            "338: split4_ir1_ov2_39.npy\n",
            "339: split4_ir1_ov2_40.npy\n",
            "340: split4_ir2_ov1_41.npy\n",
            "341: split4_ir2_ov1_42.npy\n",
            "342: split4_ir2_ov1_43.npy\n",
            "343: split4_ir2_ov1_44.npy\n",
            "344: split4_ir2_ov1_45.npy\n",
            "345: split4_ir2_ov1_46.npy\n",
            "346: split4_ir2_ov1_47.npy\n",
            "347: split4_ir2_ov1_48.npy\n",
            "348: split4_ir2_ov1_49.npy\n",
            "349: split4_ir2_ov1_50.npy\n",
            "350: split4_ir2_ov2_51.npy\n",
            "351: split4_ir2_ov2_52.npy\n",
            "352: split4_ir2_ov2_53.npy\n",
            "353: split4_ir2_ov2_54.npy\n",
            "354: split4_ir2_ov2_55.npy\n",
            "355: split4_ir2_ov2_56.npy\n",
            "356: split4_ir2_ov2_57.npy\n",
            "357: split4_ir2_ov2_58.npy\n",
            "358: split4_ir2_ov2_59.npy\n",
            "359: split4_ir2_ov2_60.npy\n",
            "360: split4_ir3_ov1_61.npy\n",
            "361: split4_ir3_ov1_62.npy\n",
            "362: split4_ir3_ov1_63.npy\n",
            "363: split4_ir3_ov1_64.npy\n",
            "364: split4_ir3_ov1_65.npy\n",
            "365: split4_ir3_ov1_66.npy\n",
            "366: split4_ir3_ov1_67.npy\n",
            "367: split4_ir3_ov1_68.npy\n",
            "368: split4_ir3_ov1_69.npy\n",
            "369: split4_ir3_ov1_70.npy\n",
            "370: split4_ir3_ov2_71.npy\n",
            "371: split4_ir3_ov2_72.npy\n",
            "372: split4_ir3_ov2_73.npy\n",
            "373: split4_ir3_ov2_74.npy\n",
            "374: split4_ir3_ov2_75.npy\n",
            "375: split4_ir3_ov2_76.npy\n",
            "376: split4_ir3_ov2_77.npy\n",
            "377: split4_ir3_ov2_78.npy\n",
            "378: split4_ir3_ov2_79.npy\n",
            "379: split4_ir3_ov2_80.npy\n",
            "380: split4_ir4_ov1_81.npy\n",
            "381: split4_ir4_ov1_82.npy\n",
            "382: split4_ir4_ov1_83.npy\n",
            "383: split4_ir4_ov1_84.npy\n",
            "384: split4_ir4_ov1_85.npy\n",
            "385: split4_ir4_ov1_86.npy\n",
            "386: split4_ir4_ov1_87.npy\n",
            "387: split4_ir4_ov1_88.npy\n",
            "388: split4_ir4_ov1_89.npy\n",
            "389: split4_ir4_ov1_90.npy\n",
            "390: split4_ir4_ov2_100.npy\n",
            "391: split4_ir4_ov2_91.npy\n",
            "392: split4_ir4_ov2_92.npy\n",
            "393: split4_ir4_ov2_93.npy\n",
            "394: split4_ir4_ov2_94.npy\n",
            "395: split4_ir4_ov2_95.npy\n",
            "396: split4_ir4_ov2_96.npy\n",
            "397: split4_ir4_ov2_97.npy\n",
            "398: split4_ir4_ov2_98.npy\n",
            "399: split4_ir4_ov2_99.npy\n",
            "Normalized_features_wts_file: D:\\sap\\sap_feat\\mic_wts. Saved.\n",
            "Normalizing feature files:\n",
            "\t\tfeat_dir_norm D:\\sap\\sap_feat\\mic_dev_norm\n",
            "0: split1_ir0_ov1_1.npy\n",
            "1: split1_ir0_ov1_10.npy\n",
            "2: split1_ir0_ov1_2.npy\n",
            "3: split1_ir0_ov1_3.npy\n",
            "4: split1_ir0_ov1_4.npy\n",
            "5: split1_ir0_ov1_5.npy\n",
            "6: split1_ir0_ov1_6.npy\n",
            "7: split1_ir0_ov1_7.npy\n",
            "8: split1_ir0_ov1_8.npy\n",
            "9: split1_ir0_ov1_9.npy\n",
            "10: split1_ir0_ov2_11.npy\n",
            "11: split1_ir0_ov2_12.npy\n",
            "12: split1_ir0_ov2_13.npy\n",
            "13: split1_ir0_ov2_14.npy\n",
            "14: split1_ir0_ov2_15.npy\n",
            "15: split1_ir0_ov2_16.npy\n",
            "16: split1_ir0_ov2_17.npy\n",
            "17: split1_ir0_ov2_18.npy\n",
            "18: split1_ir0_ov2_19.npy\n",
            "19: split1_ir0_ov2_20.npy\n",
            "20: split1_ir1_ov1_21.npy\n",
            "21: split1_ir1_ov1_22.npy\n",
            "22: split1_ir1_ov1_23.npy\n",
            "23: split1_ir1_ov1_24.npy\n",
            "24: split1_ir1_ov1_25.npy\n",
            "25: split1_ir1_ov1_26.npy\n",
            "26: split1_ir1_ov1_27.npy\n",
            "27: split1_ir1_ov1_28.npy\n",
            "28: split1_ir1_ov1_29.npy\n",
            "29: split1_ir1_ov1_30.npy\n",
            "30: split1_ir1_ov2_31.npy\n",
            "31: split1_ir1_ov2_32.npy\n",
            "32: split1_ir1_ov2_33.npy\n",
            "33: split1_ir1_ov2_34.npy\n",
            "34: split1_ir1_ov2_35.npy\n",
            "35: split1_ir1_ov2_36.npy\n",
            "36: split1_ir1_ov2_37.npy\n",
            "37: split1_ir1_ov2_38.npy\n",
            "38: split1_ir1_ov2_39.npy\n",
            "39: split1_ir1_ov2_40.npy\n",
            "40: split1_ir2_ov1_41.npy\n",
            "41: split1_ir2_ov1_42.npy\n",
            "42: split1_ir2_ov1_43.npy\n",
            "43: split1_ir2_ov1_44.npy\n",
            "44: split1_ir2_ov1_45.npy\n",
            "45: split1_ir2_ov1_46.npy\n",
            "46: split1_ir2_ov1_47.npy\n",
            "47: split1_ir2_ov1_48.npy\n",
            "48: split1_ir2_ov1_49.npy\n",
            "49: split1_ir2_ov1_50.npy\n",
            "50: split1_ir2_ov2_51.npy\n",
            "51: split1_ir2_ov2_52.npy\n",
            "52: split1_ir2_ov2_53.npy\n",
            "53: split1_ir2_ov2_54.npy\n",
            "54: split1_ir2_ov2_55.npy\n",
            "55: split1_ir2_ov2_56.npy\n",
            "56: split1_ir2_ov2_57.npy\n",
            "57: split1_ir2_ov2_58.npy\n",
            "58: split1_ir2_ov2_59.npy\n",
            "59: split1_ir2_ov2_60.npy\n",
            "60: split1_ir3_ov1_61.npy\n",
            "61: split1_ir3_ov1_62.npy\n",
            "62: split1_ir3_ov1_63.npy\n",
            "63: split1_ir3_ov1_64.npy\n",
            "64: split1_ir3_ov1_65.npy\n",
            "65: split1_ir3_ov1_66.npy\n",
            "66: split1_ir3_ov1_67.npy\n",
            "67: split1_ir3_ov1_68.npy\n",
            "68: split1_ir3_ov1_69.npy\n",
            "69: split1_ir3_ov1_70.npy\n",
            "70: split1_ir3_ov2_71.npy\n",
            "71: split1_ir3_ov2_72.npy\n",
            "72: split1_ir3_ov2_73.npy\n",
            "73: split1_ir3_ov2_74.npy\n",
            "74: split1_ir3_ov2_75.npy\n",
            "75: split1_ir3_ov2_76.npy\n",
            "76: split1_ir3_ov2_77.npy\n",
            "77: split1_ir3_ov2_78.npy\n",
            "78: split1_ir3_ov2_79.npy\n",
            "79: split1_ir3_ov2_80.npy\n",
            "80: split1_ir4_ov1_81.npy\n",
            "81: split1_ir4_ov1_82.npy\n",
            "82: split1_ir4_ov1_83.npy\n",
            "83: split1_ir4_ov1_84.npy\n",
            "84: split1_ir4_ov1_85.npy\n",
            "85: split1_ir4_ov1_86.npy\n",
            "86: split1_ir4_ov1_87.npy\n",
            "87: split1_ir4_ov1_88.npy\n",
            "88: split1_ir4_ov1_89.npy\n",
            "89: split1_ir4_ov1_90.npy\n",
            "90: split1_ir4_ov2_100.npy\n",
            "91: split1_ir4_ov2_91.npy\n",
            "92: split1_ir4_ov2_92.npy\n",
            "93: split1_ir4_ov2_93.npy\n",
            "94: split1_ir4_ov2_94.npy\n",
            "95: split1_ir4_ov2_95.npy\n",
            "96: split1_ir4_ov2_96.npy\n",
            "97: split1_ir4_ov2_97.npy\n",
            "98: split1_ir4_ov2_98.npy\n",
            "99: split1_ir4_ov2_99.npy\n",
            "100: split2_ir0_ov1_1.npy\n",
            "101: split2_ir0_ov1_10.npy\n",
            "102: split2_ir0_ov1_2.npy\n",
            "103: split2_ir0_ov1_3.npy\n",
            "104: split2_ir0_ov1_4.npy\n",
            "105: split2_ir0_ov1_5.npy\n",
            "106: split2_ir0_ov1_6.npy\n",
            "107: split2_ir0_ov1_7.npy\n",
            "108: split2_ir0_ov1_8.npy\n",
            "109: split2_ir0_ov1_9.npy\n",
            "110: split2_ir0_ov2_11.npy\n",
            "111: split2_ir0_ov2_12.npy\n",
            "112: split2_ir0_ov2_13.npy\n",
            "113: split2_ir0_ov2_14.npy\n",
            "114: split2_ir0_ov2_15.npy\n",
            "115: split2_ir0_ov2_16.npy\n",
            "116: split2_ir0_ov2_17.npy\n",
            "117: split2_ir0_ov2_18.npy\n",
            "118: split2_ir0_ov2_19.npy\n",
            "119: split2_ir0_ov2_20.npy\n",
            "120: split2_ir1_ov1_21.npy\n",
            "121: split2_ir1_ov1_22.npy\n",
            "122: split2_ir1_ov1_23.npy\n",
            "123: split2_ir1_ov1_24.npy\n",
            "124: split2_ir1_ov1_25.npy\n",
            "125: split2_ir1_ov1_26.npy\n",
            "126: split2_ir1_ov1_27.npy\n",
            "127: split2_ir1_ov1_28.npy\n",
            "128: split2_ir1_ov1_29.npy\n",
            "129: split2_ir1_ov1_30.npy\n",
            "130: split2_ir1_ov2_31.npy\n",
            "131: split2_ir1_ov2_32.npy\n",
            "132: split2_ir1_ov2_33.npy\n",
            "133: split2_ir1_ov2_34.npy\n",
            "134: split2_ir1_ov2_35.npy\n",
            "135: split2_ir1_ov2_36.npy\n",
            "136: split2_ir1_ov2_37.npy\n",
            "137: split2_ir1_ov2_38.npy\n",
            "138: split2_ir1_ov2_39.npy\n",
            "139: split2_ir1_ov2_40.npy\n",
            "140: split2_ir2_ov1_41.npy\n",
            "141: split2_ir2_ov1_42.npy\n",
            "142: split2_ir2_ov1_43.npy\n",
            "143: split2_ir2_ov1_44.npy\n",
            "144: split2_ir2_ov1_45.npy\n",
            "145: split2_ir2_ov1_46.npy\n",
            "146: split2_ir2_ov1_47.npy\n",
            "147: split2_ir2_ov1_48.npy\n",
            "148: split2_ir2_ov1_49.npy\n",
            "149: split2_ir2_ov1_50.npy\n",
            "150: split2_ir2_ov2_51.npy\n",
            "151: split2_ir2_ov2_52.npy\n",
            "152: split2_ir2_ov2_53.npy\n",
            "153: split2_ir2_ov2_54.npy\n",
            "154: split2_ir2_ov2_55.npy\n",
            "155: split2_ir2_ov2_56.npy\n",
            "156: split2_ir2_ov2_57.npy\n",
            "157: split2_ir2_ov2_58.npy\n",
            "158: split2_ir2_ov2_59.npy\n",
            "159: split2_ir2_ov2_60.npy\n",
            "160: split2_ir3_ov1_61.npy\n",
            "161: split2_ir3_ov1_62.npy\n",
            "162: split2_ir3_ov1_63.npy\n",
            "163: split2_ir3_ov1_64.npy\n",
            "164: split2_ir3_ov1_65.npy\n",
            "165: split2_ir3_ov1_66.npy\n",
            "166: split2_ir3_ov1_67.npy\n",
            "167: split2_ir3_ov1_68.npy\n",
            "168: split2_ir3_ov1_69.npy\n",
            "169: split2_ir3_ov1_70.npy\n",
            "170: split2_ir3_ov2_71.npy\n",
            "171: split2_ir3_ov2_72.npy\n",
            "172: split2_ir3_ov2_73.npy\n",
            "173: split2_ir3_ov2_74.npy\n",
            "174: split2_ir3_ov2_75.npy\n",
            "175: split2_ir3_ov2_76.npy\n",
            "176: split2_ir3_ov2_77.npy\n",
            "177: split2_ir3_ov2_78.npy\n",
            "178: split2_ir3_ov2_79.npy\n",
            "179: split2_ir3_ov2_80.npy\n",
            "180: split2_ir4_ov1_81.npy\n",
            "181: split2_ir4_ov1_82.npy\n",
            "182: split2_ir4_ov1_83.npy\n",
            "183: split2_ir4_ov1_84.npy\n",
            "184: split2_ir4_ov1_85.npy\n",
            "185: split2_ir4_ov1_86.npy\n",
            "186: split2_ir4_ov1_87.npy\n",
            "187: split2_ir4_ov1_88.npy\n",
            "188: split2_ir4_ov1_89.npy\n",
            "189: split2_ir4_ov1_90.npy\n",
            "190: split2_ir4_ov2_100.npy\n",
            "191: split2_ir4_ov2_91.npy\n",
            "192: split2_ir4_ov2_92.npy\n",
            "193: split2_ir4_ov2_93.npy\n",
            "194: split2_ir4_ov2_94.npy\n",
            "195: split2_ir4_ov2_95.npy\n",
            "196: split2_ir4_ov2_96.npy\n",
            "197: split2_ir4_ov2_97.npy\n",
            "198: split2_ir4_ov2_98.npy\n",
            "199: split2_ir4_ov2_99.npy\n",
            "200: split3_ir0_ov1_1.npy\n",
            "201: split3_ir0_ov1_10.npy\n",
            "202: split3_ir0_ov1_2.npy\n",
            "203: split3_ir0_ov1_3.npy\n",
            "204: split3_ir0_ov1_4.npy\n",
            "205: split3_ir0_ov1_5.npy\n",
            "206: split3_ir0_ov1_6.npy\n",
            "207: split3_ir0_ov1_7.npy\n",
            "208: split3_ir0_ov1_8.npy\n",
            "209: split3_ir0_ov1_9.npy\n",
            "210: split3_ir0_ov2_11.npy\n",
            "211: split3_ir0_ov2_12.npy\n",
            "212: split3_ir0_ov2_13.npy\n",
            "213: split3_ir0_ov2_14.npy\n",
            "214: split3_ir0_ov2_15.npy\n",
            "215: split3_ir0_ov2_16.npy\n",
            "216: split3_ir0_ov2_17.npy\n",
            "217: split3_ir0_ov2_18.npy\n",
            "218: split3_ir0_ov2_19.npy\n",
            "219: split3_ir0_ov2_20.npy\n",
            "220: split3_ir1_ov1_21.npy\n",
            "221: split3_ir1_ov1_22.npy\n",
            "222: split3_ir1_ov1_23.npy\n",
            "223: split3_ir1_ov1_24.npy\n",
            "224: split3_ir1_ov1_25.npy\n",
            "225: split3_ir1_ov1_26.npy\n",
            "226: split3_ir1_ov1_27.npy\n",
            "227: split3_ir1_ov1_28.npy\n",
            "228: split3_ir1_ov1_29.npy\n",
            "229: split3_ir1_ov1_30.npy\n",
            "230: split3_ir1_ov2_31.npy\n",
            "231: split3_ir1_ov2_32.npy\n",
            "232: split3_ir1_ov2_33.npy\n",
            "233: split3_ir1_ov2_34.npy\n",
            "234: split3_ir1_ov2_35.npy\n",
            "235: split3_ir1_ov2_36.npy\n",
            "236: split3_ir1_ov2_37.npy\n",
            "237: split3_ir1_ov2_38.npy\n",
            "238: split3_ir1_ov2_39.npy\n",
            "239: split3_ir1_ov2_40.npy\n",
            "240: split3_ir2_ov1_41.npy\n",
            "241: split3_ir2_ov1_42.npy\n",
            "242: split3_ir2_ov1_43.npy\n",
            "243: split3_ir2_ov1_44.npy\n",
            "244: split3_ir2_ov1_45.npy\n",
            "245: split3_ir2_ov1_46.npy\n",
            "246: split3_ir2_ov1_47.npy\n",
            "247: split3_ir2_ov1_48.npy\n",
            "248: split3_ir2_ov1_49.npy\n",
            "249: split3_ir2_ov1_50.npy\n",
            "250: split3_ir2_ov2_51.npy\n",
            "251: split3_ir2_ov2_52.npy\n",
            "252: split3_ir2_ov2_53.npy\n",
            "253: split3_ir2_ov2_54.npy\n",
            "254: split3_ir2_ov2_55.npy\n",
            "255: split3_ir2_ov2_56.npy\n",
            "256: split3_ir2_ov2_57.npy\n",
            "257: split3_ir2_ov2_58.npy\n",
            "258: split3_ir2_ov2_59.npy\n",
            "259: split3_ir2_ov2_60.npy\n",
            "260: split3_ir3_ov1_61.npy\n",
            "261: split3_ir3_ov1_62.npy\n",
            "262: split3_ir3_ov1_63.npy\n",
            "263: split3_ir3_ov1_64.npy\n",
            "264: split3_ir3_ov1_65.npy\n",
            "265: split3_ir3_ov1_66.npy\n",
            "266: split3_ir3_ov1_67.npy\n",
            "267: split3_ir3_ov1_68.npy\n",
            "268: split3_ir3_ov1_69.npy\n",
            "269: split3_ir3_ov1_70.npy\n",
            "270: split3_ir3_ov2_71.npy\n",
            "271: split3_ir3_ov2_72.npy\n",
            "272: split3_ir3_ov2_73.npy\n",
            "273: split3_ir3_ov2_74.npy\n",
            "274: split3_ir3_ov2_75.npy\n",
            "275: split3_ir3_ov2_76.npy\n",
            "276: split3_ir3_ov2_77.npy\n",
            "277: split3_ir3_ov2_78.npy\n",
            "278: split3_ir3_ov2_79.npy\n",
            "279: split3_ir3_ov2_80.npy\n",
            "280: split3_ir4_ov1_81.npy\n",
            "281: split3_ir4_ov1_82.npy\n",
            "282: split3_ir4_ov1_83.npy\n",
            "283: split3_ir4_ov1_84.npy\n",
            "284: split3_ir4_ov1_85.npy\n",
            "285: split3_ir4_ov1_86.npy\n",
            "286: split3_ir4_ov1_87.npy\n",
            "287: split3_ir4_ov1_88.npy\n",
            "288: split3_ir4_ov1_89.npy\n",
            "289: split3_ir4_ov1_90.npy\n",
            "290: split3_ir4_ov2_100.npy\n",
            "291: split3_ir4_ov2_91.npy\n",
            "292: split3_ir4_ov2_92.npy\n",
            "293: split3_ir4_ov2_93.npy\n",
            "294: split3_ir4_ov2_94.npy\n",
            "295: split3_ir4_ov2_95.npy\n",
            "296: split3_ir4_ov2_96.npy\n",
            "297: split3_ir4_ov2_97.npy\n",
            "298: split3_ir4_ov2_98.npy\n",
            "299: split3_ir4_ov2_99.npy\n",
            "300: split4_ir0_ov1_1.npy\n",
            "301: split4_ir0_ov1_10.npy\n",
            "302: split4_ir0_ov1_2.npy\n",
            "303: split4_ir0_ov1_3.npy\n",
            "304: split4_ir0_ov1_4.npy\n",
            "305: split4_ir0_ov1_5.npy\n",
            "306: split4_ir0_ov1_6.npy\n",
            "307: split4_ir0_ov1_7.npy\n",
            "308: split4_ir0_ov1_8.npy\n",
            "309: split4_ir0_ov1_9.npy\n",
            "310: split4_ir0_ov2_11.npy\n",
            "311: split4_ir0_ov2_12.npy\n",
            "312: split4_ir0_ov2_13.npy\n",
            "313: split4_ir0_ov2_14.npy\n",
            "314: split4_ir0_ov2_15.npy\n",
            "315: split4_ir0_ov2_16.npy\n",
            "316: split4_ir0_ov2_17.npy\n",
            "317: split4_ir0_ov2_18.npy\n",
            "318: split4_ir0_ov2_19.npy\n",
            "319: split4_ir0_ov2_20.npy\n",
            "320: split4_ir1_ov1_21.npy\n",
            "321: split4_ir1_ov1_22.npy\n",
            "322: split4_ir1_ov1_23.npy\n",
            "323: split4_ir1_ov1_24.npy\n",
            "324: split4_ir1_ov1_25.npy\n",
            "325: split4_ir1_ov1_26.npy\n",
            "326: split4_ir1_ov1_27.npy\n",
            "327: split4_ir1_ov1_28.npy\n",
            "328: split4_ir1_ov1_29.npy\n",
            "329: split4_ir1_ov1_30.npy\n",
            "330: split4_ir1_ov2_31.npy\n",
            "331: split4_ir1_ov2_32.npy\n",
            "332: split4_ir1_ov2_33.npy\n",
            "333: split4_ir1_ov2_34.npy\n",
            "334: split4_ir1_ov2_35.npy\n",
            "335: split4_ir1_ov2_36.npy\n",
            "336: split4_ir1_ov2_37.npy\n",
            "337: split4_ir1_ov2_38.npy\n",
            "338: split4_ir1_ov2_39.npy\n",
            "339: split4_ir1_ov2_40.npy\n",
            "340: split4_ir2_ov1_41.npy\n",
            "341: split4_ir2_ov1_42.npy\n",
            "342: split4_ir2_ov1_43.npy\n",
            "343: split4_ir2_ov1_44.npy\n",
            "344: split4_ir2_ov1_45.npy\n",
            "345: split4_ir2_ov1_46.npy\n",
            "346: split4_ir2_ov1_47.npy\n",
            "347: split4_ir2_ov1_48.npy\n",
            "348: split4_ir2_ov1_49.npy\n",
            "349: split4_ir2_ov1_50.npy\n",
            "350: split4_ir2_ov2_51.npy\n",
            "351: split4_ir2_ov2_52.npy\n",
            "352: split4_ir2_ov2_53.npy\n",
            "353: split4_ir2_ov2_54.npy\n",
            "354: split4_ir2_ov2_55.npy\n",
            "355: split4_ir2_ov2_56.npy\n",
            "356: split4_ir2_ov2_57.npy\n",
            "357: split4_ir2_ov2_58.npy\n",
            "358: split4_ir2_ov2_59.npy\n",
            "359: split4_ir2_ov2_60.npy\n",
            "360: split4_ir3_ov1_61.npy\n",
            "361: split4_ir3_ov1_62.npy\n",
            "362: split4_ir3_ov1_63.npy\n",
            "363: split4_ir3_ov1_64.npy\n",
            "364: split4_ir3_ov1_65.npy\n",
            "365: split4_ir3_ov1_66.npy\n",
            "366: split4_ir3_ov1_67.npy\n",
            "367: split4_ir3_ov1_68.npy\n",
            "368: split4_ir3_ov1_69.npy\n",
            "369: split4_ir3_ov1_70.npy\n",
            "370: split4_ir3_ov2_71.npy\n",
            "371: split4_ir3_ov2_72.npy\n",
            "372: split4_ir3_ov2_73.npy\n",
            "373: split4_ir3_ov2_74.npy\n",
            "374: split4_ir3_ov2_75.npy\n",
            "375: split4_ir3_ov2_76.npy\n",
            "376: split4_ir3_ov2_77.npy\n",
            "377: split4_ir3_ov2_78.npy\n",
            "378: split4_ir3_ov2_79.npy\n",
            "379: split4_ir3_ov2_80.npy\n",
            "380: split4_ir4_ov1_81.npy\n",
            "381: split4_ir4_ov1_82.npy\n",
            "382: split4_ir4_ov1_83.npy\n",
            "383: split4_ir4_ov1_84.npy\n",
            "384: split4_ir4_ov1_85.npy\n",
            "385: split4_ir4_ov1_86.npy\n",
            "386: split4_ir4_ov1_87.npy\n",
            "387: split4_ir4_ov1_88.npy\n",
            "388: split4_ir4_ov1_89.npy\n",
            "389: split4_ir4_ov1_90.npy\n",
            "390: split4_ir4_ov2_100.npy\n",
            "391: split4_ir4_ov2_91.npy\n",
            "392: split4_ir4_ov2_92.npy\n",
            "393: split4_ir4_ov2_93.npy\n",
            "394: split4_ir4_ov2_94.npy\n",
            "395: split4_ir4_ov2_95.npy\n",
            "396: split4_ir4_ov2_96.npy\n",
            "397: split4_ir4_ov2_97.npy\n",
            "398: split4_ir4_ov2_98.npy\n",
            "399: split4_ir4_ov2_99.npy\n",
            "normalized files written to D:\\sap\\sap_feat\\mic_dev_norm\n",
            "Extracting labels:\n",
            "\t\taud_dir D:\\sap\\mic_dev\n",
            "\t\tdesc_dir D:\\sap\\metadata_dev\n",
            "\t\tlabel_dir D:\\sap\\sap_feat\\mic_dev_label\n",
            "D:\\sap\\sap_feat\\mic_dev_label folder does not exist, creating it.\n",
            "0: split1_ir0_ov1_1.csv\n",
            "1: split1_ir0_ov1_10.csv\n",
            "2: split1_ir0_ov1_2.csv\n",
            "3: split1_ir0_ov1_3.csv\n",
            "4: split1_ir0_ov1_4.csv\n",
            "5: split1_ir0_ov1_5.csv\n",
            "6: split1_ir0_ov1_6.csv\n",
            "7: split1_ir0_ov1_7.csv\n",
            "8: split1_ir0_ov1_8.csv\n",
            "9: split1_ir0_ov1_9.csv\n",
            "10: split1_ir0_ov2_11.csv\n",
            "11: split1_ir0_ov2_12.csv\n",
            "12: split1_ir0_ov2_13.csv\n",
            "13: split1_ir0_ov2_14.csv\n",
            "14: split1_ir0_ov2_15.csv\n",
            "15: split1_ir0_ov2_16.csv\n",
            "16: split1_ir0_ov2_17.csv\n",
            "17: split1_ir0_ov2_18.csv\n",
            "18: split1_ir0_ov2_19.csv\n",
            "19: split1_ir0_ov2_20.csv\n",
            "20: split1_ir1_ov1_21.csv\n",
            "21: split1_ir1_ov1_22.csv\n",
            "22: split1_ir1_ov1_23.csv\n",
            "23: split1_ir1_ov1_24.csv\n",
            "24: split1_ir1_ov1_25.csv\n",
            "25: split1_ir1_ov1_26.csv\n",
            "26: split1_ir1_ov1_27.csv\n",
            "27: split1_ir1_ov1_28.csv\n",
            "28: split1_ir1_ov1_29.csv\n",
            "29: split1_ir1_ov1_30.csv\n",
            "30: split1_ir1_ov2_31.csv\n",
            "31: split1_ir1_ov2_32.csv\n",
            "32: split1_ir1_ov2_33.csv\n",
            "33: split1_ir1_ov2_34.csv\n",
            "34: split1_ir1_ov2_35.csv\n",
            "35: split1_ir1_ov2_36.csv\n",
            "36: split1_ir1_ov2_37.csv\n",
            "37: split1_ir1_ov2_38.csv\n",
            "38: split1_ir1_ov2_39.csv\n",
            "39: split1_ir1_ov2_40.csv\n",
            "40: split1_ir2_ov1_41.csv\n",
            "41: split1_ir2_ov1_42.csv\n",
            "42: split1_ir2_ov1_43.csv\n",
            "43: split1_ir2_ov1_44.csv\n",
            "44: split1_ir2_ov1_45.csv\n",
            "45: split1_ir2_ov1_46.csv\n",
            "46: split1_ir2_ov1_47.csv\n",
            "47: split1_ir2_ov1_48.csv\n",
            "48: split1_ir2_ov1_49.csv\n",
            "49: split1_ir2_ov1_50.csv\n",
            "50: split1_ir2_ov2_51.csv\n",
            "51: split1_ir2_ov2_52.csv\n",
            "52: split1_ir2_ov2_53.csv\n",
            "53: split1_ir2_ov2_54.csv\n",
            "54: split1_ir2_ov2_55.csv\n",
            "55: split1_ir2_ov2_56.csv\n",
            "56: split1_ir2_ov2_57.csv\n",
            "57: split1_ir2_ov2_58.csv\n",
            "58: split1_ir2_ov2_59.csv\n",
            "59: split1_ir2_ov2_60.csv\n",
            "60: split1_ir3_ov1_61.csv\n",
            "61: split1_ir3_ov1_62.csv\n",
            "62: split1_ir3_ov1_63.csv\n",
            "63: split1_ir3_ov1_64.csv\n",
            "64: split1_ir3_ov1_65.csv\n",
            "65: split1_ir3_ov1_66.csv\n",
            "66: split1_ir3_ov1_67.csv\n",
            "67: split1_ir3_ov1_68.csv\n",
            "68: split1_ir3_ov1_69.csv\n",
            "69: split1_ir3_ov1_70.csv\n",
            "70: split1_ir3_ov2_71.csv\n",
            "71: split1_ir3_ov2_72.csv\n",
            "72: split1_ir3_ov2_73.csv\n",
            "73: split1_ir3_ov2_74.csv\n",
            "74: split1_ir3_ov2_75.csv\n",
            "75: split1_ir3_ov2_76.csv\n",
            "76: split1_ir3_ov2_77.csv\n",
            "77: split1_ir3_ov2_78.csv\n",
            "78: split1_ir3_ov2_79.csv\n",
            "79: split1_ir3_ov2_80.csv\n",
            "80: split1_ir4_ov1_81.csv\n",
            "81: split1_ir4_ov1_82.csv\n",
            "82: split1_ir4_ov1_83.csv\n",
            "83: split1_ir4_ov1_84.csv\n",
            "84: split1_ir4_ov1_85.csv\n",
            "85: split1_ir4_ov1_86.csv\n",
            "86: split1_ir4_ov1_87.csv\n",
            "87: split1_ir4_ov1_88.csv\n",
            "88: split1_ir4_ov1_89.csv\n",
            "89: split1_ir4_ov1_90.csv\n",
            "90: split1_ir4_ov2_100.csv\n",
            "91: split1_ir4_ov2_91.csv\n",
            "92: split1_ir4_ov2_92.csv\n",
            "93: split1_ir4_ov2_93.csv\n",
            "94: split1_ir4_ov2_94.csv\n",
            "95: split1_ir4_ov2_95.csv\n",
            "96: split1_ir4_ov2_96.csv\n",
            "97: split1_ir4_ov2_97.csv\n",
            "98: split1_ir4_ov2_98.csv\n",
            "99: split1_ir4_ov2_99.csv\n",
            "100: split2_ir0_ov1_1.csv\n",
            "101: split2_ir0_ov1_10.csv\n",
            "102: split2_ir0_ov1_2.csv\n",
            "103: split2_ir0_ov1_3.csv\n",
            "104: split2_ir0_ov1_4.csv\n",
            "105: split2_ir0_ov1_5.csv\n",
            "106: split2_ir0_ov1_6.csv\n",
            "107: split2_ir0_ov1_7.csv\n",
            "108: split2_ir0_ov1_8.csv\n",
            "109: split2_ir0_ov1_9.csv\n",
            "110: split2_ir0_ov2_11.csv\n",
            "111: split2_ir0_ov2_12.csv\n",
            "112: split2_ir0_ov2_13.csv\n",
            "113: split2_ir0_ov2_14.csv\n",
            "114: split2_ir0_ov2_15.csv\n",
            "115: split2_ir0_ov2_16.csv\n",
            "116: split2_ir0_ov2_17.csv\n",
            "117: split2_ir0_ov2_18.csv\n",
            "118: split2_ir0_ov2_19.csv\n",
            "119: split2_ir0_ov2_20.csv\n",
            "120: split2_ir1_ov1_21.csv\n",
            "121: split2_ir1_ov1_22.csv\n",
            "122: split2_ir1_ov1_23.csv\n",
            "123: split2_ir1_ov1_24.csv\n",
            "124: split2_ir1_ov1_25.csv\n",
            "125: split2_ir1_ov1_26.csv\n",
            "126: split2_ir1_ov1_27.csv\n",
            "127: split2_ir1_ov1_28.csv\n",
            "128: split2_ir1_ov1_29.csv\n",
            "129: split2_ir1_ov1_30.csv\n",
            "130: split2_ir1_ov2_31.csv\n",
            "131: split2_ir1_ov2_32.csv\n",
            "132: split2_ir1_ov2_33.csv\n",
            "133: split2_ir1_ov2_34.csv\n",
            "134: split2_ir1_ov2_35.csv\n",
            "135: split2_ir1_ov2_36.csv\n",
            "136: split2_ir1_ov2_37.csv\n",
            "137: split2_ir1_ov2_38.csv\n",
            "138: split2_ir1_ov2_39.csv\n",
            "139: split2_ir1_ov2_40.csv\n",
            "140: split2_ir2_ov1_41.csv\n",
            "141: split2_ir2_ov1_42.csv\n",
            "142: split2_ir2_ov1_43.csv\n",
            "143: split2_ir2_ov1_44.csv\n",
            "144: split2_ir2_ov1_45.csv\n",
            "145: split2_ir2_ov1_46.csv\n",
            "146: split2_ir2_ov1_47.csv\n",
            "147: split2_ir2_ov1_48.csv\n",
            "148: split2_ir2_ov1_49.csv\n",
            "149: split2_ir2_ov1_50.csv\n",
            "150: split2_ir2_ov2_51.csv\n",
            "151: split2_ir2_ov2_52.csv\n",
            "152: split2_ir2_ov2_53.csv\n",
            "153: split2_ir2_ov2_54.csv\n",
            "154: split2_ir2_ov2_55.csv\n",
            "155: split2_ir2_ov2_56.csv\n",
            "156: split2_ir2_ov2_57.csv\n",
            "157: split2_ir2_ov2_58.csv\n",
            "158: split2_ir2_ov2_59.csv\n",
            "159: split2_ir2_ov2_60.csv\n",
            "160: split2_ir3_ov1_61.csv\n",
            "161: split2_ir3_ov1_62.csv\n",
            "162: split2_ir3_ov1_63.csv\n",
            "163: split2_ir3_ov1_64.csv\n",
            "164: split2_ir3_ov1_65.csv\n",
            "165: split2_ir3_ov1_66.csv\n",
            "166: split2_ir3_ov1_67.csv\n",
            "167: split2_ir3_ov1_68.csv\n",
            "168: split2_ir3_ov1_69.csv\n",
            "169: split2_ir3_ov1_70.csv\n",
            "170: split2_ir3_ov2_71.csv\n",
            "171: split2_ir3_ov2_72.csv\n",
            "172: split2_ir3_ov2_73.csv\n",
            "173: split2_ir3_ov2_74.csv\n",
            "174: split2_ir3_ov2_75.csv\n",
            "175: split2_ir3_ov2_76.csv\n",
            "176: split2_ir3_ov2_77.csv\n",
            "177: split2_ir3_ov2_78.csv\n",
            "178: split2_ir3_ov2_79.csv\n",
            "179: split2_ir3_ov2_80.csv\n",
            "180: split2_ir4_ov1_81.csv\n",
            "181: split2_ir4_ov1_82.csv\n",
            "182: split2_ir4_ov1_83.csv\n",
            "183: split2_ir4_ov1_84.csv\n",
            "184: split2_ir4_ov1_85.csv\n",
            "185: split2_ir4_ov1_86.csv\n",
            "186: split2_ir4_ov1_87.csv\n",
            "187: split2_ir4_ov1_88.csv\n",
            "188: split2_ir4_ov1_89.csv\n",
            "189: split2_ir4_ov1_90.csv\n",
            "190: split2_ir4_ov2_100.csv\n",
            "191: split2_ir4_ov2_91.csv\n",
            "192: split2_ir4_ov2_92.csv\n",
            "193: split2_ir4_ov2_93.csv\n",
            "194: split2_ir4_ov2_94.csv\n",
            "195: split2_ir4_ov2_95.csv\n",
            "196: split2_ir4_ov2_96.csv\n",
            "197: split2_ir4_ov2_97.csv\n",
            "198: split2_ir4_ov2_98.csv\n",
            "199: split2_ir4_ov2_99.csv\n",
            "200: split3_ir0_ov1_1.csv\n",
            "201: split3_ir0_ov1_10.csv\n",
            "202: split3_ir0_ov1_2.csv\n",
            "203: split3_ir0_ov1_3.csv\n",
            "204: split3_ir0_ov1_4.csv\n",
            "205: split3_ir0_ov1_5.csv\n",
            "206: split3_ir0_ov1_6.csv\n",
            "207: split3_ir0_ov1_7.csv\n",
            "208: split3_ir0_ov1_8.csv\n",
            "209: split3_ir0_ov1_9.csv\n",
            "210: split3_ir0_ov2_11.csv\n",
            "211: split3_ir0_ov2_12.csv\n",
            "212: split3_ir0_ov2_13.csv\n",
            "213: split3_ir0_ov2_14.csv\n",
            "214: split3_ir0_ov2_15.csv\n",
            "215: split3_ir0_ov2_16.csv\n",
            "216: split3_ir0_ov2_17.csv\n",
            "217: split3_ir0_ov2_18.csv\n",
            "218: split3_ir0_ov2_19.csv\n",
            "219: split3_ir0_ov2_20.csv\n",
            "220: split3_ir1_ov1_21.csv\n",
            "221: split3_ir1_ov1_22.csv\n",
            "222: split3_ir1_ov1_23.csv\n",
            "223: split3_ir1_ov1_24.csv\n",
            "224: split3_ir1_ov1_25.csv\n",
            "225: split3_ir1_ov1_26.csv\n",
            "226: split3_ir1_ov1_27.csv\n",
            "227: split3_ir1_ov1_28.csv\n",
            "228: split3_ir1_ov1_29.csv\n",
            "229: split3_ir1_ov1_30.csv\n",
            "230: split3_ir1_ov2_31.csv\n",
            "231: split3_ir1_ov2_32.csv\n",
            "232: split3_ir1_ov2_33.csv\n",
            "233: split3_ir1_ov2_34.csv\n",
            "234: split3_ir1_ov2_35.csv\n",
            "235: split3_ir1_ov2_36.csv\n",
            "236: split3_ir1_ov2_37.csv\n",
            "237: split3_ir1_ov2_38.csv\n",
            "238: split3_ir1_ov2_39.csv\n",
            "239: split3_ir1_ov2_40.csv\n",
            "240: split3_ir2_ov1_41.csv\n",
            "241: split3_ir2_ov1_42.csv\n",
            "242: split3_ir2_ov1_43.csv\n",
            "243: split3_ir2_ov1_44.csv\n",
            "244: split3_ir2_ov1_45.csv\n",
            "245: split3_ir2_ov1_46.csv\n",
            "246: split3_ir2_ov1_47.csv\n",
            "247: split3_ir2_ov1_48.csv\n",
            "248: split3_ir2_ov1_49.csv\n",
            "249: split3_ir2_ov1_50.csv\n",
            "250: split3_ir2_ov2_51.csv\n",
            "251: split3_ir2_ov2_52.csv\n",
            "252: split3_ir2_ov2_53.csv\n",
            "253: split3_ir2_ov2_54.csv\n",
            "254: split3_ir2_ov2_55.csv\n",
            "255: split3_ir2_ov2_56.csv\n",
            "256: split3_ir2_ov2_57.csv\n",
            "257: split3_ir2_ov2_58.csv\n",
            "258: split3_ir2_ov2_59.csv\n",
            "259: split3_ir2_ov2_60.csv\n",
            "260: split3_ir3_ov1_61.csv\n",
            "261: split3_ir3_ov1_62.csv\n",
            "262: split3_ir3_ov1_63.csv\n",
            "263: split3_ir3_ov1_64.csv\n",
            "264: split3_ir3_ov1_65.csv\n",
            "265: split3_ir3_ov1_66.csv\n",
            "266: split3_ir3_ov1_67.csv\n",
            "267: split3_ir3_ov1_68.csv\n",
            "268: split3_ir3_ov1_69.csv\n",
            "269: split3_ir3_ov1_70.csv\n",
            "270: split3_ir3_ov2_71.csv\n",
            "271: split3_ir3_ov2_72.csv\n",
            "272: split3_ir3_ov2_73.csv\n",
            "273: split3_ir3_ov2_74.csv\n",
            "274: split3_ir3_ov2_75.csv\n",
            "275: split3_ir3_ov2_76.csv\n",
            "276: split3_ir3_ov2_77.csv\n",
            "277: split3_ir3_ov2_78.csv\n",
            "278: split3_ir3_ov2_79.csv\n",
            "279: split3_ir3_ov2_80.csv\n",
            "280: split3_ir4_ov1_81.csv\n",
            "281: split3_ir4_ov1_82.csv\n",
            "282: split3_ir4_ov1_83.csv\n",
            "283: split3_ir4_ov1_84.csv\n",
            "284: split3_ir4_ov1_85.csv\n",
            "285: split3_ir4_ov1_86.csv\n",
            "286: split3_ir4_ov1_87.csv\n",
            "287: split3_ir4_ov1_88.csv\n",
            "288: split3_ir4_ov1_89.csv\n",
            "289: split3_ir4_ov1_90.csv\n",
            "290: split3_ir4_ov2_100.csv\n",
            "291: split3_ir4_ov2_91.csv\n",
            "292: split3_ir4_ov2_92.csv\n",
            "293: split3_ir4_ov2_93.csv\n",
            "294: split3_ir4_ov2_94.csv\n",
            "295: split3_ir4_ov2_95.csv\n",
            "296: split3_ir4_ov2_96.csv\n",
            "297: split3_ir4_ov2_97.csv\n",
            "298: split3_ir4_ov2_98.csv\n",
            "299: split3_ir4_ov2_99.csv\n",
            "300: split4_ir0_ov1_1.csv\n",
            "301: split4_ir0_ov1_10.csv\n",
            "302: split4_ir0_ov1_2.csv\n",
            "303: split4_ir0_ov1_3.csv\n",
            "304: split4_ir0_ov1_4.csv\n",
            "305: split4_ir0_ov1_5.csv\n",
            "306: split4_ir0_ov1_6.csv\n",
            "307: split4_ir0_ov1_7.csv\n",
            "308: split4_ir0_ov1_8.csv\n",
            "309: split4_ir0_ov1_9.csv\n",
            "310: split4_ir0_ov2_11.csv\n",
            "311: split4_ir0_ov2_12.csv\n",
            "312: split4_ir0_ov2_13.csv\n",
            "313: split4_ir0_ov2_14.csv\n",
            "314: split4_ir0_ov2_15.csv\n",
            "315: split4_ir0_ov2_16.csv\n",
            "316: split4_ir0_ov2_17.csv\n",
            "317: split4_ir0_ov2_18.csv\n",
            "318: split4_ir0_ov2_19.csv\n",
            "319: split4_ir0_ov2_20.csv\n",
            "320: split4_ir1_ov1_21.csv\n",
            "321: split4_ir1_ov1_22.csv\n",
            "322: split4_ir1_ov1_23.csv\n",
            "323: split4_ir1_ov1_24.csv\n",
            "324: split4_ir1_ov1_25.csv\n",
            "325: split4_ir1_ov1_26.csv\n",
            "326: split4_ir1_ov1_27.csv\n",
            "327: split4_ir1_ov1_28.csv\n",
            "328: split4_ir1_ov1_29.csv\n",
            "329: split4_ir1_ov1_30.csv\n",
            "330: split4_ir1_ov2_31.csv\n",
            "331: split4_ir1_ov2_32.csv\n",
            "332: split4_ir1_ov2_33.csv\n",
            "333: split4_ir1_ov2_34.csv\n",
            "334: split4_ir1_ov2_35.csv\n",
            "335: split4_ir1_ov2_36.csv\n",
            "336: split4_ir1_ov2_37.csv\n",
            "337: split4_ir1_ov2_38.csv\n",
            "338: split4_ir1_ov2_39.csv\n",
            "339: split4_ir1_ov2_40.csv\n",
            "340: split4_ir2_ov1_41.csv\n",
            "341: split4_ir2_ov1_42.csv\n",
            "342: split4_ir2_ov1_43.csv\n",
            "343: split4_ir2_ov1_44.csv\n",
            "344: split4_ir2_ov1_45.csv\n",
            "345: split4_ir2_ov1_46.csv\n",
            "346: split4_ir2_ov1_47.csv\n",
            "347: split4_ir2_ov1_48.csv\n",
            "348: split4_ir2_ov1_49.csv\n",
            "349: split4_ir2_ov1_50.csv\n",
            "350: split4_ir2_ov2_51.csv\n",
            "351: split4_ir2_ov2_52.csv\n",
            "352: split4_ir2_ov2_53.csv\n",
            "353: split4_ir2_ov2_54.csv\n",
            "354: split4_ir2_ov2_55.csv\n",
            "355: split4_ir2_ov2_56.csv\n",
            "356: split4_ir2_ov2_57.csv\n",
            "357: split4_ir2_ov2_58.csv\n",
            "358: split4_ir2_ov2_59.csv\n",
            "359: split4_ir2_ov2_60.csv\n",
            "360: split4_ir3_ov1_61.csv\n",
            "361: split4_ir3_ov1_62.csv\n",
            "362: split4_ir3_ov1_63.csv\n",
            "363: split4_ir3_ov1_64.csv\n",
            "364: split4_ir3_ov1_65.csv\n",
            "365: split4_ir3_ov1_66.csv\n",
            "366: split4_ir3_ov1_67.csv\n",
            "367: split4_ir3_ov1_68.csv\n",
            "368: split4_ir3_ov1_69.csv\n",
            "369: split4_ir3_ov1_70.csv\n",
            "370: split4_ir3_ov2_71.csv\n",
            "371: split4_ir3_ov2_72.csv\n",
            "372: split4_ir3_ov2_73.csv\n",
            "373: split4_ir3_ov2_74.csv\n",
            "374: split4_ir3_ov2_75.csv\n",
            "375: split4_ir3_ov2_76.csv\n",
            "376: split4_ir3_ov2_77.csv\n",
            "377: split4_ir3_ov2_78.csv\n",
            "378: split4_ir3_ov2_79.csv\n",
            "379: split4_ir3_ov2_80.csv\n",
            "380: split4_ir4_ov1_81.csv\n",
            "381: split4_ir4_ov1_82.csv\n",
            "382: split4_ir4_ov1_83.csv\n",
            "383: split4_ir4_ov1_84.csv\n",
            "384: split4_ir4_ov1_85.csv\n",
            "385: split4_ir4_ov1_86.csv\n",
            "386: split4_ir4_ov1_87.csv\n",
            "387: split4_ir4_ov1_88.csv\n",
            "388: split4_ir4_ov1_89.csv\n",
            "389: split4_ir4_ov1_90.csv\n",
            "390: split4_ir4_ov2_100.csv\n",
            "391: split4_ir4_ov2_91.csv\n",
            "392: split4_ir4_ov2_92.csv\n",
            "393: split4_ir4_ov2_93.csv\n",
            "394: split4_ir4_ov2_94.csv\n",
            "395: split4_ir4_ov2_95.csv\n",
            "396: split4_ir4_ov2_96.csv\n",
            "397: split4_ir4_ov2_97.csv\n",
            "398: split4_ir4_ov2_98.csv\n",
            "399: split4_ir4_ov2_99.csv\n",
            "D:\\sap\\sap_feat\\mic_eval folder does not exist, creating it.\n",
            "Extracting spectrogram:\n",
            "\t\taud_dir D:\\sap\\mic_eval\n",
            "\t\tdesc_dir None\n",
            "\t\tfeat_dir D:\\sap\\sap_feat\\mic_eval\n",
            "0: split0_1.wav\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<ipython-input-5-f8170aedd1de>:49: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._eps = np.spacing(np.float(1e-16))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1: split0_10.wav\n",
            "2: split0_100.wav\n",
            "3: split0_11.wav\n",
            "4: split0_12.wav\n",
            "5: split0_13.wav\n",
            "6: split0_14.wav\n",
            "7: split0_15.wav\n",
            "8: split0_16.wav\n",
            "9: split0_17.wav\n",
            "10: split0_18.wav\n",
            "11: split0_19.wav\n",
            "12: split0_2.wav\n",
            "13: split0_20.wav\n",
            "14: split0_21.wav\n",
            "15: split0_22.wav\n",
            "16: split0_23.wav\n",
            "17: split0_24.wav\n",
            "18: split0_25.wav\n",
            "19: split0_26.wav\n",
            "20: split0_27.wav\n",
            "21: split0_28.wav\n",
            "22: split0_29.wav\n",
            "23: split0_3.wav\n",
            "24: split0_30.wav\n",
            "25: split0_31.wav\n",
            "26: split0_32.wav\n",
            "27: split0_33.wav\n",
            "28: split0_34.wav\n",
            "29: split0_35.wav\n",
            "30: split0_36.wav\n",
            "31: split0_37.wav\n",
            "32: split0_38.wav\n",
            "33: split0_39.wav\n",
            "34: split0_4.wav\n",
            "35: split0_40.wav\n",
            "36: split0_41.wav\n",
            "37: split0_42.wav\n",
            "38: split0_43.wav\n",
            "39: split0_44.wav\n",
            "40: split0_45.wav\n",
            "41: split0_46.wav\n",
            "42: split0_47.wav\n",
            "43: split0_48.wav\n",
            "44: split0_49.wav\n",
            "45: split0_5.wav\n",
            "46: split0_50.wav\n",
            "47: split0_51.wav\n",
            "48: split0_52.wav\n",
            "49: split0_53.wav\n",
            "50: split0_54.wav\n",
            "51: split0_55.wav\n",
            "52: split0_56.wav\n",
            "53: split0_57.wav\n",
            "54: split0_58.wav\n",
            "55: split0_59.wav\n",
            "56: split0_6.wav\n",
            "57: split0_60.wav\n",
            "58: split0_61.wav\n",
            "59: split0_62.wav\n",
            "60: split0_63.wav\n",
            "61: split0_64.wav\n",
            "62: split0_65.wav\n",
            "63: split0_66.wav\n",
            "64: split0_67.wav\n",
            "65: split0_68.wav\n",
            "66: split0_69.wav\n",
            "67: split0_7.wav\n",
            "68: split0_70.wav\n",
            "69: split0_71.wav\n",
            "70: split0_72.wav\n",
            "71: split0_73.wav\n",
            "72: split0_74.wav\n",
            "73: split0_75.wav\n",
            "74: split0_76.wav\n",
            "75: split0_77.wav\n",
            "76: split0_78.wav\n",
            "77: split0_79.wav\n",
            "78: split0_8.wav\n",
            "79: split0_80.wav\n",
            "80: split0_81.wav\n",
            "81: split0_82.wav\n",
            "82: split0_83.wav\n",
            "83: split0_84.wav\n",
            "84: split0_85.wav\n",
            "85: split0_86.wav\n",
            "86: split0_87.wav\n",
            "87: split0_88.wav\n",
            "88: split0_89.wav\n",
            "89: split0_9.wav\n",
            "90: split0_90.wav\n",
            "91: split0_91.wav\n",
            "92: split0_92.wav\n",
            "93: split0_93.wav\n",
            "94: split0_94.wav\n",
            "95: split0_95.wav\n",
            "96: split0_96.wav\n",
            "97: split0_97.wav\n",
            "98: split0_98.wav\n",
            "99: split0_99.wav\n",
            "D:\\sap\\sap_feat\\mic_eval_norm folder does not exist, creating it.\n",
            "Normalized_features_wts_file: D:\\sap\\sap_feat\\mic_wts. Loaded.\n",
            "Normalizing feature files:\n",
            "\t\tfeat_dir_norm D:\\sap\\sap_feat\\mic_eval_norm\n",
            "0: split0_1.npy\n",
            "1: split0_10.npy\n",
            "2: split0_100.npy\n",
            "3: split0_11.npy\n",
            "4: split0_12.npy\n",
            "5: split0_13.npy\n",
            "6: split0_14.npy\n",
            "7: split0_15.npy\n",
            "8: split0_16.npy\n",
            "9: split0_17.npy\n",
            "10: split0_18.npy\n",
            "11: split0_19.npy\n",
            "12: split0_2.npy\n",
            "13: split0_20.npy\n",
            "14: split0_21.npy\n",
            "15: split0_22.npy\n",
            "16: split0_23.npy\n",
            "17: split0_24.npy\n",
            "18: split0_25.npy\n",
            "19: split0_26.npy\n",
            "20: split0_27.npy\n",
            "21: split0_28.npy\n",
            "22: split0_29.npy\n",
            "23: split0_3.npy\n",
            "24: split0_30.npy\n",
            "25: split0_31.npy\n",
            "26: split0_32.npy\n",
            "27: split0_33.npy\n",
            "28: split0_34.npy\n",
            "29: split0_35.npy\n",
            "30: split0_36.npy\n",
            "31: split0_37.npy\n",
            "32: split0_38.npy\n",
            "33: split0_39.npy\n",
            "34: split0_4.npy\n",
            "35: split0_40.npy\n",
            "36: split0_41.npy\n",
            "37: split0_42.npy\n",
            "38: split0_43.npy\n",
            "39: split0_44.npy\n",
            "40: split0_45.npy\n",
            "41: split0_46.npy\n",
            "42: split0_47.npy\n",
            "43: split0_48.npy\n",
            "44: split0_49.npy\n",
            "45: split0_5.npy\n",
            "46: split0_50.npy\n",
            "47: split0_51.npy\n",
            "48: split0_52.npy\n",
            "49: split0_53.npy\n",
            "50: split0_54.npy\n",
            "51: split0_55.npy\n",
            "52: split0_56.npy\n",
            "53: split0_57.npy\n",
            "54: split0_58.npy\n",
            "55: split0_59.npy\n",
            "56: split0_6.npy\n",
            "57: split0_60.npy\n",
            "58: split0_61.npy\n",
            "59: split0_62.npy\n",
            "60: split0_63.npy\n",
            "61: split0_64.npy\n",
            "62: split0_65.npy\n",
            "63: split0_66.npy\n",
            "64: split0_67.npy\n",
            "65: split0_68.npy\n",
            "66: split0_69.npy\n",
            "67: split0_7.npy\n",
            "68: split0_70.npy\n",
            "69: split0_71.npy\n",
            "70: split0_72.npy\n",
            "71: split0_73.npy\n",
            "72: split0_74.npy\n",
            "73: split0_75.npy\n",
            "74: split0_76.npy\n",
            "75: split0_77.npy\n",
            "76: split0_78.npy\n",
            "77: split0_79.npy\n",
            "78: split0_8.npy\n",
            "79: split0_80.npy\n",
            "80: split0_81.npy\n",
            "81: split0_82.npy\n",
            "82: split0_83.npy\n",
            "83: split0_84.npy\n",
            "84: split0_85.npy\n",
            "85: split0_86.npy\n",
            "86: split0_87.npy\n",
            "87: split0_88.npy\n",
            "88: split0_89.npy\n",
            "89: split0_9.npy\n",
            "90: split0_90.npy\n",
            "91: split0_91.npy\n",
            "92: split0_92.npy\n",
            "93: split0_93.npy\n",
            "94: split0_94.npy\n",
            "95: split0_95.npy\n",
            "96: split0_96.npy\n",
            "97: split0_97.npy\n",
            "98: split0_98.npy\n",
            "99: split0_99.npy\n",
            "normalized files written to D:\\sap\\sap_feat\\mic_eval_norm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3swHPH5wXPcn"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxHTFFaMXVcv",
        "tags": []
      },
      "source": [
        "\n",
        "def get_params(argv):\n",
        "    print(\"SET: {}\".format(argv))\n",
        "    params = dict(\n",
        "        quick_test=False,     \n",
        "\n",
        "        # INPUT PATH\n",
        "        dataset_dir='D:\\sap',  \n",
        "\n",
        "        # OUTPUT PATH\n",
        "        feat_label_dir='D:\\sap\\sap_feat',  \n",
        "        model_dir='D:\\sap\\model_lab',   \n",
        "        dcase_output=True,     \n",
        "                               \n",
        "        dcase_dir='D:\\sap\\seld-dcase2019-master\\dcase',  \n",
        "\n",
        "        # DATASET LOADING PARAMETERS\n",
        "        mode='dev',         \n",
        "        dataset='mic',       \n",
        "\n",
        "        # DNN MODEL PARAMETERS\n",
        "        sequence_length=128,        # Feature sequence length\n",
        "        batch_size=16,              # Batch size\n",
        "        dropout_rate=0,             # Dropout rate, constant for all layers\n",
        "        nb_cnn2d_filt=64,           # Number of CNN nodes, constant for each layer\n",
        "        pool_size=[8, 8, 4],        # CNN pooling, length of list = number of CNN layers, list value = pooling per layer\n",
        "        rnn_size=[128, 128],        # RNN contents, length of list = number of layers, list value = number of nodes\n",
        "        fnn_size=[128],             # FNN contents, length of list = number of layers, list value = number of nodes\n",
        "        loss_weights=[1., 50.],     # [sed, doa] weight for scaling the DNN outputs\n",
        "        nb_epochs=20,               # Train for maximum epochs\n",
        "        epochs_per_fit=5,           # Number of epochs per fit\n",
        "\n",
        "    )\n",
        "    params['patience'] = int(0.1 * params['nb_epochs'])\n",
        "    \n",
        "    if argv == '1':\n",
        "        print(\"USING DEFAULT PARAMETERS\\n\")\n",
        "\n",
        "    elif argv == '2':\n",
        "        params['mode'] = 'dev'\n",
        "        params['dataset'] = 'mic'\n",
        "\n",
        "    elif argv == '3':\n",
        "        params['mode'] = 'eval'\n",
        "        params['dataset'] = 'mic'\n",
        "\n",
        "    elif argv == '4':\n",
        "        params['mode'] = 'dev'\n",
        "        params['dataset'] = 'foa'\n",
        "\n",
        "    elif argv == '5':\n",
        "        params['mode'] = 'eval'\n",
        "        params['dataset'] = 'foa'\n",
        "\n",
        "    # Quick test\n",
        "    elif argv == '999':\n",
        "        print(\"QUICK TEST MODE\\n\")\n",
        "        params['quick_test'] = True\n",
        "        params['epochs_per_fit'] = 1\n",
        "\n",
        "    else:\n",
        "        print('ERROR: unknown argument {}'.format(argv))\n",
        "        exit()\n",
        "\n",
        "    for key, value in params.items():\n",
        "        print(\"\\t{}: {}\".format(key, value))\n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWUqTrP0Xn31"
      },
      "source": [
        "# Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr64EjP8Xp_J",
        "tags": []
      },
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from IPython import embed\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "\n",
        "class DataGenerator(object):\n",
        "    def __init__(\n",
        "            self, dataset='mic', feat_label_dir='', is_eval=False, split=1, batch_size=32, seq_len=64,\n",
        "            shuffle=True, per_file=False\n",
        "    ):\n",
        "        self._per_file = per_file\n",
        "        self._is_eval = is_eval\n",
        "        self._splits = np.array(split)\n",
        "        self._batch_size = batch_size\n",
        "        self._seq_len = seq_len\n",
        "        self._shuffle = shuffle\n",
        "        self._feat_cls = FeatureClass(feat_label_dir=feat_label_dir, dataset=dataset, is_eval=is_eval)\n",
        "        self._label_dir = self._feat_cls.get_label_dir()\n",
        "        self._feat_dir = self._feat_cls.get_normalized_feat_dir()\n",
        "\n",
        "        self._filenames_list = list()\n",
        "        self._nb_frames_file = 0     \n",
        "        self._feat_len = None\n",
        "        self._2_nb_ch = 2 * self._feat_cls.get_nb_channels()\n",
        "        self._label_len = None  \n",
        "        self._doa_len = None    \n",
        "        self._class_dict = self._feat_cls.get_classes()\n",
        "        self._nb_classes = len(self._class_dict.keys())\n",
        "        self._default_azi, self._default_ele = self._feat_cls.get_default_azi_ele_regr()\n",
        "        self._get_filenames_list_and_feat_label_sizes()\n",
        "\n",
        "        self._batch_seq_len = self._batch_size*self._seq_len\n",
        "        self._circ_buf_feat = None\n",
        "        self._circ_buf_label = None\n",
        "\n",
        "        if self._per_file:\n",
        "            self._nb_total_batches = len(self._filenames_list)\n",
        "        else:\n",
        "            self._nb_total_batches = int(np.floor((len(self._filenames_list) * self._nb_frames_file /\n",
        "                                               float(self._seq_len * self._batch_size))))\n",
        "\n",
        "        # self._dummy_feat_vec = np.ones(self._feat_len.shape) *\n",
        "\n",
        "        print(\n",
        "            '\\tDatagen_mode: {}, nb_files: {}, nb_classes:{}\\n'\n",
        "            '\\tnb_frames_file: {}, feat_len: {}, nb_ch: {}, label_len:{}\\n'.format(\n",
        "                'eval' if self._is_eval else 'dev', len(self._filenames_list),  self._nb_classes,\n",
        "                self._nb_frames_file, self._feat_len, self._2_nb_ch, self._label_len\n",
        "                )\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            '\\tDataset: {}, split: {}\\n'\n",
        "            '\\tbatch_size: {}, seq_len: {}, shuffle: {}\\n'\n",
        "            '\\tlabel_dir: {}\\n '\n",
        "            '\\tfeat_dir: {}\\n'.format(\n",
        "                dataset, split,\n",
        "                self._batch_size, self._seq_len, self._shuffle,\n",
        "                self._label_dir, self._feat_dir\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def get_data_sizes(self):\n",
        "        feat_shape = (self._batch_size, self._2_nb_ch, self._seq_len, self._feat_len)\n",
        "        if self._is_eval:\n",
        "            label_shape = None\n",
        "        else:\n",
        "            label_shape = [\n",
        "                (self._batch_size, self._seq_len, self._nb_classes),\n",
        "                (self._batch_size, self._seq_len, self._nb_classes*2)\n",
        "            ]\n",
        "        return feat_shape, label_shape\n",
        "\n",
        "    def get_total_batches_in_data(self):\n",
        "        return self._nb_total_batches\n",
        "\n",
        "    def _get_filenames_list_and_feat_label_sizes(self):\n",
        "        for filename in os.listdir(self._feat_dir):\n",
        "            if int(filename[5]) in self._splits: \n",
        "                self._filenames_list.append(filename)\n",
        "\n",
        "        temp_feat = np.load(os.path.join(self._feat_dir, self._filenames_list[0]))\n",
        "        self._nb_frames_file = temp_feat.shape[0]\n",
        "        self._feat_len = temp_feat.shape[1] // self._2_nb_ch\n",
        "\n",
        "        if not self._is_eval:\n",
        "            temp_label = np.load(os.path.join(self._label_dir, self._filenames_list[0]))\n",
        "            self._label_len = temp_label.shape[-1]\n",
        "            self._doa_len = (self._label_len - self._nb_classes)//self._nb_classes\n",
        "\n",
        "        if self._per_file:\n",
        "            self._batch_size = int(np.ceil(temp_feat.shape[0]/float(self._seq_len)))\n",
        "\n",
        "        return\n",
        "\n",
        "    def generate(self):\n",
        "        \"\"\"\n",
        "        Generates batches of samples\n",
        "        :return: \n",
        "        \"\"\"\n",
        "\n",
        "        while 1:\n",
        "            if self._shuffle:\n",
        "                random.shuffle(self._filenames_list)\n",
        "\n",
        "            \n",
        "            self._circ_buf_feat = deque()\n",
        "            self._circ_buf_label = deque()\n",
        "\n",
        "            file_cnt = 0\n",
        "            if self._is_eval:\n",
        "                for i in range(self._nb_total_batches):\n",
        "                    \n",
        "                    while len(self._circ_buf_feat) < self._batch_seq_len:\n",
        "                        temp_feat = np.load(os.path.join(self._feat_dir, self._filenames_list[file_cnt]))\n",
        "\n",
        "                        for row_cnt, row in enumerate(temp_feat):\n",
        "                            self._circ_buf_feat.append(row)\n",
        "\n",
        "                        \n",
        "                        if self._per_file:\n",
        "                            extra_frames = self._batch_seq_len - temp_feat.shape[0]\n",
        "                            extra_feat = np.ones((extra_frames, temp_feat.shape[1])) * 1e-6\n",
        "\n",
        "                            for row_cnt, row in enumerate(extra_feat):\n",
        "                                self._circ_buf_feat.append(row)\n",
        "\n",
        "                        file_cnt = file_cnt + 1\n",
        "\n",
        "                    \n",
        "                    feat = np.zeros((self._batch_seq_len, self._feat_len * self._2_nb_ch))\n",
        "                    for j in range(self._batch_seq_len):\n",
        "                        feat[j, :] = self._circ_buf_feat.popleft()\n",
        "                    feat = np.reshape(feat, (self._batch_seq_len, self._feat_len, self._2_nb_ch))\n",
        "\n",
        "                    \n",
        "                    feat = self._split_in_seqs(feat)\n",
        "                    feat = np.transpose(feat, (0, 3, 1, 2))\n",
        "\n",
        "                    yield feat\n",
        "\n",
        "            else:\n",
        "                for i in range(self._nb_total_batches):\n",
        "\n",
        "                \n",
        "                    while len(self._circ_buf_feat) < self._batch_seq_len:\n",
        "                        temp_feat = np.load(os.path.join(self._feat_dir, self._filenames_list[file_cnt]))\n",
        "                        temp_label = np.load(os.path.join(self._label_dir, self._filenames_list[file_cnt]))\n",
        "\n",
        "                        for row_cnt, row in enumerate(temp_feat):\n",
        "                            self._circ_buf_feat.append(row)\n",
        "                            self._circ_buf_label.append(temp_label[row_cnt])\n",
        "\n",
        "                        \n",
        "                        if self._per_file:\n",
        "                            extra_frames = self._batch_seq_len - temp_feat.shape[0]\n",
        "                            extra_feat = np.ones((extra_frames, temp_feat.shape[1])) * 1e-6\n",
        "\n",
        "                            extra_labels = np.zeros((extra_frames, temp_label.shape[1]))\n",
        "                            extra_labels[:, self._nb_classes:2 * self._nb_classes] = self._default_azi\n",
        "                            extra_labels[:, 2 * self._nb_classes:] = self._default_ele\n",
        "\n",
        "                            for row_cnt, row in enumerate(extra_feat):\n",
        "                                self._circ_buf_feat.append(row)\n",
        "                                self._circ_buf_label.append(extra_labels[row_cnt])\n",
        "\n",
        "                        file_cnt = file_cnt + 1\n",
        "\n",
        "                    \n",
        "                    feat = np.zeros((self._batch_seq_len, self._feat_len * self._2_nb_ch))\n",
        "                    label = np.zeros((self._batch_seq_len, self._label_len))\n",
        "                    for j in range(self._batch_seq_len):\n",
        "                        feat[j, :] = self._circ_buf_feat.popleft()\n",
        "                        label[j, :] = self._circ_buf_label.popleft()\n",
        "                    feat = np.reshape(feat, (self._batch_seq_len, self._feat_len, self._2_nb_ch))\n",
        "\n",
        "                  \n",
        "                    feat = self._split_in_seqs(feat)\n",
        "                    feat = np.transpose(feat, (0, 3, 1, 2))\n",
        "                    label = self._split_in_seqs(label)\n",
        "\n",
        "                  \n",
        "                    azi_rad = label[:, :, self._nb_classes:2 * self._nb_classes] * np.pi / 180\n",
        "                   \n",
        "\n",
        "                    \n",
        "                    ele_rad = label[:, :, 2 * self._nb_classes:] * np.pi / self._default_ele\n",
        "\n",
        "                    label = [\n",
        "                        label[:, :, :self._nb_classes],  \n",
        "                        np.concatenate((azi_rad, ele_rad), -1)  \n",
        "                         ]\n",
        "\n",
        "                    yield feat, label\n",
        "\n",
        "    def _split_in_seqs(self, data):\n",
        "        if len(data.shape) == 1:\n",
        "            if data.shape[0] % self._seq_len:\n",
        "                data = data[:-(data.shape[0] % self._seq_len), :]\n",
        "            data = data.reshape((data.shape[0] // self._seq_len, self._seq_len, 1))\n",
        "        elif len(data.shape) == 2:\n",
        "            if data.shape[0] % self._seq_len:\n",
        "                data = data[:-(data.shape[0] % self._seq_len), :]\n",
        "            data = data.reshape((data.shape[0] // self._seq_len, self._seq_len, data.shape[1]))\n",
        "        elif len(data.shape) == 3:\n",
        "            if data.shape[0] % self._seq_len:\n",
        "                data = data[:-(data.shape[0] % self._seq_len), :, :]\n",
        "            data = data.reshape((data.shape[0] // self._seq_len, self._seq_len, data.shape[1], data.shape[2]))\n",
        "        else:\n",
        "            print('ERROR: Unknown data dimensions: {}'.format(data.shape))\n",
        "            exit()\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def split_multi_channels(data, num_channels):\n",
        "        tmp = None\n",
        "        in_shape = data.shape\n",
        "        if len(in_shape) == 3:\n",
        "            hop = in_shape[2] / num_channels\n",
        "            tmp = np.zeros((in_shape[0], num_channels, in_shape[1], hop))\n",
        "            for i in range(num_channels):\n",
        "                tmp[:, i, :, :] = data[:, :, i * hop:(i + 1) * hop]\n",
        "        elif len(in_shape) == 4 and num_channels == 1:\n",
        "            tmp = np.zeros((in_shape[0], 1, in_shape[1], in_shape[2], in_shape[3]))\n",
        "            tmp[:, 0, :, :, :] = data\n",
        "        else:\n",
        "            print('ERROR: The input should be a 3D matrix but it seems to have dimensions: {}'.format(in_shape))\n",
        "            exit()\n",
        "        return tmp\n",
        "\n",
        "    def get_default_elevation(self):\n",
        "        return self._default_ele\n",
        "\n",
        "    def get_azi_ele_list(self):\n",
        "        return self._feat_cls.get_azi_ele_list()\n",
        "\n",
        "    def get_list_index(self, azi, ele):\n",
        "        return self._feat_cls.get_list_index(azi, ele)\n",
        "\n",
        "    def get_matrix_index(self, ind):\n",
        "        return self._feat_cls.get_matrix_index(ind)\n",
        "\n",
        "    def get_nb_classes(self):\n",
        "        return self._nb_classes\n",
        "\n",
        "    def nb_frames_1s(self):\n",
        "        return self._feat_cls.nb_frames_1s()\n",
        "\n",
        "    def get_hop_len_sec(self):\n",
        "        return self._feat_cls.get_hop_len_sec()\n",
        "\n",
        "    def get_classes(self):\n",
        "        return self._feat_cls.get_classes()\n",
        "    \n",
        "    def get_filelist(self):\n",
        "        return self._filenames_list\n",
        "\n",
        "    def get_frame_per_file(self):\n",
        "        return self._batch_seq_len\n",
        "\n",
        "    def get_nb_frames(self):\n",
        "        return self._feat_cls._max_frames\n",
        "\n",
        "    def get_nb_frames(self):\n",
        "        return self._feat_cls.get_nb_frames()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fpTLf2HXtxv"
      },
      "source": [
        "# Keras model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "OwiYe3d-XxDK",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "outputId": "7fcb0379-ca6d-4c1b-c311-e07a1fa67a5b"
      },
      "source": [
        "#\n",
        "# The SELDnet architecture\n",
        "#\n",
        "\n",
        "from keras.layers import Bidirectional, Conv2D, MaxPooling2D, Input\n",
        "from keras.layers.core import Dense, Activation, Dropout, Reshape, Permute\n",
        "from keras.layers.recurrent import GRU\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.layers.wrappers import TimeDistributed\n",
        "from keras.optimizers import Adam\n",
        "import keras\n",
        "keras.backend.set_image_data_format('channels_first')\n",
        "from IPython import embed\n",
        "\n",
        "\n",
        "def get_model(data_in, data_out, dropout_rate, nb_cnn2d_filt, pool_size,\n",
        "                                rnn_size, fnn_size, weights):\n",
        "    # model definition\n",
        "    s = 0\n",
        "    spec_start = Input(shape=(data_in[-3], data_in[-2], data_in[-1]))\n",
        "    # CNN\n",
        "    spec_cnn = spec_start\n",
        "    for i, convCnt in enumerate(pool_size):\n",
        "        spec_cnn = Conv2D(filters=nb_cnn2d_filt, kernel_size=(3, 3), padding='same')(spec_cnn)\n",
        "        spec_cnn = BatchNormalization()(spec_cnn)\n",
        "        spec_cnn = Activation('relu')(spec_cnn)\n",
        "        spec_cnn = MaxPooling2D(pool_size=(1, pool_size[i]))(spec_cnn)\n",
        "        spec_cnn = Dropout(dropout_rate)(spec_cnn)\n",
        "    spec_cnn = Permute((2, 1, 3))(spec_cnn)\n",
        "\n",
        "    # RNN\n",
        "    spec_rnn = Reshape((data_in[-2], -1))(spec_cnn)\n",
        "    for nb_rnn_filt in rnn_size:\n",
        "        spec_rnn = Bidirectional(\n",
        "            GRU(nb_rnn_filt, activation='tanh', dropout=dropout_rate, recurrent_dropout=dropout_rate,\n",
        "                return_sequences=True),\n",
        "            merge_mode='mul'\n",
        "        )(spec_rnn)\n",
        "\n",
        "    # FC - DOA\n",
        "    doa = spec_rnn\n",
        "    for nb_fnn_filt in fnn_size:\n",
        "        doa = TimeDistributed(Dense(nb_fnn_filt))(doa)\n",
        "        doa = Dropout(dropout_rate)(doa)\n",
        "\n",
        "    doa = TimeDistributed(Dense(data_out[1][-1]))(doa)\n",
        "    doa = Activation('linear', name='doa_out')(doa)\n",
        "\n",
        "    # FC - SED\n",
        "    sed = spec_rnn\n",
        "    for nb_fnn_filt in fnn_size:\n",
        "        sed = TimeDistributed(Dense(nb_fnn_filt))(sed)\n",
        "        sed = Dropout(dropout_rate)(sed)\n",
        "    sed = TimeDistributed(Dense(data_out[0][-1]))(sed)\n",
        "    sed = Activation('sigmoid', name='sed_out')(sed)\n",
        "\n",
        "    model = Model(inputs=spec_start, outputs=[sed, doa])\n",
        "    model.compile(optimizer=Adam(), loss=['binary_crossentropy', 'mse'], loss_weights=weights)\n",
        "\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "c:\\users\\abhin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "c:\\users\\abhin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "c:\\users\\abhin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "c:\\users\\abhin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "c:\\users\\abhin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "c:\\users\\abhin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I76V21pXX0kL",
        "tags": []
      },
      "source": [
        "# Evaluation metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS5p3PZPX8-0",
        "tags": []
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from IPython import embed\n",
        "eps = np.finfo(np.float).eps\n",
        "\n",
        "class evaluation_metrics(object):\n",
        "    def __init__(self, nb_frames_1s=None, data_gen=None):\n",
        "        # SED params\n",
        "        self._S = 0\n",
        "        self._D = 0\n",
        "        self._I = 0\n",
        "        self._TP = 0\n",
        "        self._Nref = 0\n",
        "        self._Nsys = 0\n",
        "        self._block_size = nb_frames_1s\n",
        "\n",
        "        # DOA params\n",
        "        self._doa_loss_pred_cnt = 0\n",
        "        self._nb_frames = 0\n",
        "\n",
        "        self._doa_loss_pred = 0\n",
        "        self._nb_good_pks = 0\n",
        "\n",
        "        self._data_gen = data_gen\n",
        "\n",
        "        self._less_est_cnt, self._less_est_frame_cnt = 0, 0\n",
        "        self._more_est_cnt, self._more_est_frame_cnt = 0, 0\n",
        "\n",
        "    def f1_overall_framewise(self, O, T):\n",
        "        TP = ((2 * T - O) == 1).sum()\n",
        "        Nref, Nsys = T.sum(), O.sum()\n",
        "        self._TP += TP\n",
        "        self._Nref += Nref\n",
        "        self._Nsys += Nsys\n",
        "\n",
        "    def er_overall_framewise(self, O, T):\n",
        "        FP = np.logical_and(T == 0, O == 1).sum(1)\n",
        "        FN = np.logical_and(T == 1, O == 0).sum(1)\n",
        "        S = np.minimum(FP, FN).sum()\n",
        "        D = np.maximum(0, FN - FP).sum()\n",
        "        I = np.maximum(0, FP - FN).sum()\n",
        "        self._S += S\n",
        "        self._D += D\n",
        "        self._I += I\n",
        "\n",
        "    def f1_overall_1sec(self, O, T):        \n",
        "        new_size = int(np.ceil(float(O.shape[0]) / self._block_size))\n",
        "        O_block = np.zeros((new_size, O.shape[1]))\n",
        "        T_block = np.zeros((new_size, O.shape[1]))\n",
        "        for i in range(0, new_size):\n",
        "            O_block[i, :] = np.max(O[int(i * self._block_size):int(i * self._block_size + self._block_size - 1), :], axis=0)\n",
        "            T_block[i, :] = np.max(T[int(i * self._block_size):int(i * self._block_size + self._block_size - 1), :], axis=0)\n",
        "        return self.f1_overall_framewise(O_block, T_block)\n",
        "\n",
        "    def er_overall_1sec(self, O, T):        \n",
        "        new_size = int(np.ceil(float(O.shape[0]) / self._block_size))\n",
        "        O_block = np.zeros((new_size, O.shape[1]))\n",
        "        T_block = np.zeros((new_size, O.shape[1]))\n",
        "        for i in range(0, new_size):\n",
        "            O_block[i, :] = np.max(O[int(i * self._block_size):int(i * self._block_size + self._block_size - 1), :], axis=0)\n",
        "            T_block[i, :] = np.max(T[int(i * self._block_size):int(i * self._block_size + self._block_size - 1), :], axis=0)\n",
        "        return self.er_overall_framewise(O_block, T_block)\n",
        "\n",
        "    def update_sed_scores(self, pred, gt):\n",
        "        self.f1_overall_1sec(pred, gt)\n",
        "        self.er_overall_1sec(pred, gt)\n",
        "\n",
        "    def compute_sed_scores(self):\n",
        "        ER = (self._S + self._D + self._I) / (self._Nref + 0.0)\n",
        "    \n",
        "        prec = float(self._TP) / float(self._Nsys + eps)\n",
        "        recall = float(self._TP) / float(self._Nref + eps)\n",
        "        F = 2 * prec * recall / (prec + recall + eps)\n",
        "\n",
        "        return ER, F\n",
        "\n",
        "    def update_doa_scores(self, pred_doa_thresholded, gt_doa):\n",
        "        self._doa_loss_pred_cnt += np.sum(pred_doa_thresholded)\n",
        "        self._nb_frames += pred_doa_thresholded.shape[0]\n",
        "\n",
        "        for frame in range(pred_doa_thresholded.shape[0]):\n",
        "            nb_gt_peaks = int(np.sum(gt_doa[frame, :]))\n",
        "            nb_pred_peaks = int(np.sum(pred_doa_thresholded[frame, :]))\n",
        "\n",
        "            if nb_gt_peaks == nb_pred_peaks:\n",
        "                self._nb_good_pks += 1\n",
        "            elif nb_gt_peaks > nb_pred_peaks:\n",
        "                self._less_est_frame_cnt += 1\n",
        "                self._less_est_cnt += (nb_gt_peaks - nb_pred_peaks)\n",
        "            elif nb_pred_peaks > nb_gt_peaks:\n",
        "                self._more_est_frame_cnt += 1\n",
        "                self._more_est_cnt += (nb_pred_peaks - nb_gt_peaks)\n",
        "\n",
        "            \n",
        "            if nb_gt_peaks and nb_pred_peaks:\n",
        "                pred_ind = np.where(pred_doa_thresholded[frame] == 1)[1]\n",
        "                pred_list_rad = np.array(self._data_gen .get_matrix_index(pred_ind)) * np.pi / 180\n",
        "\n",
        "                gt_ind = np.where(gt_doa[frame] == 1)[1]\n",
        "                gt_list_rad = np.array(self._data_gen .get_matrix_index(gt_ind)) * np.pi / 180\n",
        "\n",
        "                frame_dist = distance_between_gt_pred(gt_list_rad.T, pred_list_rad.T)\n",
        "                self._doa_loss_pred += frame_dist\n",
        "\n",
        "    def compute_doa_scores(self):\n",
        "        doa_error = self._doa_loss_pred / self._doa_loss_pred_cnt\n",
        "        frame_recall = self._nb_good_pks / float(self._nb_frames)\n",
        "        return doa_error, frame_recall\n",
        "\n",
        "    def reset(self):\n",
        "        # SED params\n",
        "        self._S = 0\n",
        "        self._D = 0\n",
        "        self._I = 0\n",
        "        self._TP = 0\n",
        "        self._Nref = 0\n",
        "        self._Nsys = 0\n",
        "\n",
        "        # DOA params\n",
        "        self._doa_loss_pred_cnt = 0\n",
        "        self._nb_frames = 0\n",
        "\n",
        "        self._doa_loss_pred = 0\n",
        "        self._nb_good_pks = 0\n",
        "\n",
        "        self._less_est_cnt, self._less_est_frame_cnt = 0, 0\n",
        "        self._more_est_cnt, self._more_est_frame_cnt = 0, 0\n",
        "\n",
        "\n",
        "\n",
        "def reshape_3Dto2D(A):\n",
        "    return A.reshape(A.shape[0] * A.shape[1], A.shape[2])\n",
        "\n",
        "\n",
        "def f1_overall_framewise(O, T):\n",
        "    if len(O.shape) == 3:\n",
        "        O, T = reshape_3Dto2D(O), reshape_3Dto2D(T)\n",
        "    TP = ((2 * T - O) == 1).sum()\n",
        "    Nref, Nsys = T.sum(), O.sum()\n",
        "\n",
        "    prec = float(TP) / float(Nsys + eps)\n",
        "    recall = float(TP) / float(Nref + eps)\n",
        "    f1_score = 2 * prec * recall / (prec + recall + eps)\n",
        "    return f1_score\n",
        "\n",
        "\n",
        "def er_overall_framewise(O, T):\n",
        "    if len(O.shape) == 3:\n",
        "        O, T = reshape_3Dto2D(O), reshape_3Dto2D(T)\n",
        "\n",
        "    FP = np.logical_and(T == 0, O == 1).sum(1)\n",
        "    FN = np.logical_and(T == 1, O == 0).sum(1)\n",
        "\n",
        "    S = np.minimum(FP, FN).sum()\n",
        "    D = np.maximum(0, FN-FP).sum()\n",
        "    I = np.maximum(0, FP-FN).sum()\n",
        "\n",
        "    Nref = T.sum()\n",
        "    ER = (S+D+I) / (Nref + 0.0)\n",
        "    return ER\n",
        "\n",
        "\n",
        "def f1_overall_1sec(O, T, block_size):\n",
        "    if len(O.shape) == 3:\n",
        "        O, T = reshape_3Dto2D(O), reshape_3Dto2D(T)\n",
        "    new_size = int(np.ceil(float(O.shape[0]) / block_size))\n",
        "    O_block = np.zeros((new_size, O.shape[1]))\n",
        "    T_block = np.zeros((new_size, O.shape[1]))\n",
        "    for i in range(0, new_size):\n",
        "        O_block[i, :] = np.max(O[int(i * block_size):int(i * block_size + block_size - 1), :], axis=0)\n",
        "        T_block[i, :] = np.max(T[int(i * block_size):int(i * block_size + block_size - 1), :], axis=0)\n",
        "    return f1_overall_framewise(O_block, T_block)\n",
        "\n",
        "\n",
        "def er_overall_1sec(O, T, block_size):\n",
        "    if len(O.shape) == 3:\n",
        "        O, T = reshape_3Dto2D(O), reshape_3Dto2D(T)\n",
        "    new_size = int(np.ceil(float(O.shape[0]) / block_size))\n",
        "    O_block = np.zeros((new_size, O.shape[1]))\n",
        "    T_block = np.zeros((new_size, O.shape[1]))\n",
        "    for i in range(0, new_size):\n",
        "        O_block[i, :] = np.max(O[int(i * block_size):int(i * block_size + block_size - 1), :], axis=0)\n",
        "        T_block[i, :] = np.max(T[int(i * block_size):int(i * block_size + block_size - 1), :], axis=0)\n",
        "    return er_overall_framewise(O_block, T_block)\n",
        "\n",
        "\n",
        "def compute_sed_scores(pred, gt, nb_frames_1s):\n",
        "    f1o = f1_overall_1sec(pred, gt, nb_frames_1s)\n",
        "    ero = er_overall_1sec(pred, gt, nb_frames_1s)\n",
        "    scores = [ero, f1o]\n",
        "    return scores\n",
        "\n",
        "\n",
        "def compute_doa_scores_regr(pred_doa_rad, gt_doa_rad, pred_sed, gt_sed):\n",
        "\n",
        "    nb_src_gt_list = np.zeros(gt_doa_rad.shape[0]).astype(int)\n",
        "    nb_src_pred_list = np.zeros(gt_doa_rad.shape[0]).astype(int)\n",
        "    good_frame_cnt = 0\n",
        "    doa_loss_pred = 0.0\n",
        "    nb_sed = gt_sed.shape[-1]\n",
        "\n",
        "    less_est_cnt, less_est_frame_cnt = 0, 0\n",
        "    more_est_cnt, more_est_frame_cnt = 0, 0\n",
        "\n",
        "    for frame_cnt, sed_frame in enumerate(gt_sed):\n",
        "        nb_src_gt_list[frame_cnt] = int(np.sum(sed_frame))\n",
        "        nb_src_pred_list[frame_cnt] = int(np.sum(pred_sed[frame_cnt]))\n",
        "\n",
        "      \n",
        "        if nb_src_gt_list[frame_cnt] == nb_src_pred_list[frame_cnt]:\n",
        "            good_frame_cnt = good_frame_cnt + 1\n",
        "        elif nb_src_gt_list[frame_cnt] > nb_src_pred_list[frame_cnt]:\n",
        "            less_est_cnt = less_est_cnt + nb_src_gt_list[frame_cnt] - nb_src_pred_list[frame_cnt]\n",
        "            less_est_frame_cnt = less_est_frame_cnt + 1\n",
        "        elif nb_src_gt_list[frame_cnt] < nb_src_pred_list[frame_cnt]:\n",
        "            more_est_cnt = more_est_cnt + nb_src_pred_list[frame_cnt] - nb_src_gt_list[frame_cnt]\n",
        "            more_est_frame_cnt = more_est_frame_cnt + 1\n",
        "\n",
        "        \n",
        "        if nb_src_gt_list[frame_cnt] and nb_src_pred_list[frame_cnt]:\n",
        "            \n",
        "            sed_frame_gt = gt_sed[frame_cnt]\n",
        "            doa_frame_gt_azi = gt_doa_rad[frame_cnt][:nb_sed][sed_frame_gt == 1]\n",
        "            doa_frame_gt_ele = gt_doa_rad[frame_cnt][nb_sed:][sed_frame_gt == 1]\n",
        "\n",
        "            sed_frame_pred = pred_sed[frame_cnt]\n",
        "            doa_frame_pred_azi = pred_doa_rad[frame_cnt][:nb_sed][sed_frame_pred == 1]\n",
        "            doa_frame_pred_ele = pred_doa_rad[frame_cnt][nb_sed:][sed_frame_pred == 1]\n",
        "\n",
        "            doa_loss_pred += distance_between_gt_pred(np.vstack((doa_frame_gt_azi, doa_frame_gt_ele)).T,\n",
        "                                                      np.vstack((doa_frame_pred_azi, doa_frame_pred_ele)).T)\n",
        "\n",
        "    doa_loss_pred_cnt = np.sum(nb_src_pred_list)\n",
        "    if doa_loss_pred_cnt:\n",
        "        doa_loss_pred /= doa_loss_pred_cnt\n",
        "\n",
        "    frame_recall = good_frame_cnt / float(gt_sed.shape[0])\n",
        "    er_metric = [doa_loss_pred, frame_recall, doa_loss_pred_cnt, good_frame_cnt, more_est_cnt, less_est_cnt]\n",
        "    return er_metric\n",
        "\n",
        "\n",
        "def compute_doa_scores_clas(pred_doa_thresholded, gt_doa, data_gen_test):\n",
        "    doa_loss_pred_cnt = np.sum(pred_doa_thresholded)\n",
        "\n",
        "    doa_loss_pred = 0\n",
        "    nb_good_pks = 0\n",
        "\n",
        "    less_est_cnt, less_est_frame_cnt = 0, 0\n",
        "    more_est_cnt, more_est_frame_cnt = 0, 0\n",
        "\n",
        "    for frame in range(pred_doa_thresholded.shape[0]):\n",
        "        nb_gt_peaks = int(np.sum(gt_doa[frame, :]))\n",
        "        nb_pred_peaks = int(np.sum(pred_doa_thresholded[frame, :]))\n",
        "\n",
        "        \n",
        "        if nb_gt_peaks == nb_pred_peaks:\n",
        "            nb_good_pks += 1\n",
        "        elif nb_gt_peaks > nb_pred_peaks:\n",
        "            less_est_frame_cnt += 1\n",
        "            less_est_cnt += (nb_gt_peaks - nb_pred_peaks)\n",
        "        elif nb_pred_peaks > nb_gt_peaks:\n",
        "            more_est_frame_cnt += 1\n",
        "            more_est_cnt += (nb_pred_peaks - nb_gt_peaks)\n",
        "\n",
        "        \n",
        "        if nb_gt_peaks and nb_pred_peaks:\n",
        "            pred_ind = np.where(pred_doa_thresholded[frame] == 1)[1]\n",
        "            pred_list_rad = np.array(data_gen_test.get_matrix_index(pred_ind)) * np.pi / 180\n",
        "\n",
        "            gt_ind = np.where(gt_doa[frame] == 1)[1]\n",
        "            gt_list_rad = np.array(data_gen_test.get_matrix_index(gt_ind)) * np.pi / 180\n",
        "\n",
        "            frame_dist = distance_between_gt_pred(gt_list_rad.T, pred_list_rad.T)\n",
        "            doa_loss_pred += frame_dist\n",
        "\n",
        "    if doa_loss_pred_cnt:\n",
        "        doa_loss_pred /= doa_loss_pred_cnt\n",
        "\n",
        "    frame_recall = nb_good_pks / float(pred_doa_thresholded.shape[0])\n",
        "    er_metric = [doa_loss_pred, frame_recall, doa_loss_pred_cnt, nb_good_pks, more_est_cnt, less_est_cnt]\n",
        "    return er_metric\n",
        "\n",
        "\n",
        "def distance_between_gt_pred(gt_list_rad, pred_list_rad):\n",
        "    gt_len, pred_len = gt_list_rad.shape[0], pred_list_rad.shape[0]\n",
        "    ind_pairs = np.array([[x, y] for y in range(pred_len) for x in range(gt_len)])\n",
        "    cost_mat = np.zeros((gt_len, pred_len))\n",
        "\n",
        "    if gt_len and pred_len:\n",
        "        az1, ele1, az2, ele2 = gt_list_rad[ind_pairs[:, 0], 0], gt_list_rad[ind_pairs[:, 0], 1], \\\n",
        "                               pred_list_rad[ind_pairs[:, 1], 0], pred_list_rad[ind_pairs[:, 1], 1]\n",
        "        cost_mat[ind_pairs[:, 0], ind_pairs[:, 1]] = distance_between_spherical_coordinates_rad(az1, ele1, az2, ele2)\n",
        "\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_mat)\n",
        "    cost = cost_mat[row_ind, col_ind].sum()\n",
        "    return cost\n",
        "\n",
        "\n",
        "def distance_between_spherical_coordinates_rad(az1, ele1, az2, ele2):\n",
        "    dist = np.sin(ele1) * np.sin(ele2) + np.cos(ele1) * np.cos(ele2) * np.cos(np.abs(az1 - az2))\n",
        "    \n",
        "    dist = np.clip(dist, -1, 1)\n",
        "    dist = np.arccos(dist) * 180 / np.pi\n",
        "    return dist\n",
        "\n",
        "\n",
        "def distance_between_cartesian_coordinates(x1, y1, z1, x2, y2, z2):\n",
        "    dist = np.sqrt((x1-x2) ** 2 + (y1-y2) ** 2 + (z1-z2) ** 2)\n",
        "    dist = 2 * np.arcsin(dist / 2.0) * 180/np.pi\n",
        "    return dist\n",
        "\n",
        "\n",
        "def sph2cart(azimuth, elevation, r):\n",
        "\n",
        "    x = r * np.cos(elevation) * np.cos(azimuth)\n",
        "    y = r * np.cos(elevation) * np.sin(azimuth)\n",
        "    z = r * np.sin(elevation)\n",
        "    return x, y, z\n",
        "\n",
        "\n",
        "def cart2sph(x, y, z):\n",
        "\n",
        "    azimuth = np.arctan2(y,x)\n",
        "    elevation = np.arctan2(z,np.sqrt(x**2 + y**2))\n",
        "    r = np.sqrt(x**2 + y**2 + z**2)\n",
        "    return azimuth, elevation, r\n",
        "\n",
        "\n",
        "###############################################################\n",
        "# SELD scoring functions\n",
        "###############################################################\n",
        "\n",
        "\n",
        "def compute_seld_metric(sed_error, doa_error):\n",
        "    seld_metric = np.mean([\n",
        "        sed_error[0],\n",
        "        1 - sed_error[1],\n",
        "        doa_error[0]/180,\n",
        "        1 - doa_error[1]]\n",
        "        )\n",
        "    return seld_metric\n",
        "\n",
        "\n",
        "def compute_seld_metrics_from_output_format_dict(_pred_dict, _gt_dict, _feat_cls):\n",
        "    _gt_labels = output_format_dict_to_classification_labels(_gt_dict, _feat_cls)\n",
        "    _pred_labels = output_format_dict_to_classification_labels(_pred_dict, _feat_cls)\n",
        "\n",
        "    _er, _f = compute_sed_scores(_pred_labels.max(2), _gt_labels.max(2), _feat_cls.nb_frames_1s())\n",
        "    _doa_err, _frame_recall, d1, d2, d3, d4 = compute_doa_scores_clas(_pred_labels, _gt_labels, _feat_cls)\n",
        "    _seld_scr = compute_seld_metric([_er, _f], [_doa_err, _frame_recall])\n",
        "    return _seld_scr, _er, _f, _doa_err, _frame_recall\n",
        "\n",
        "\n",
        "\n",
        "def output_format_dict_to_classification_labels(_output_dict, _feat_cls):\n",
        "\n",
        "    _unique_classes = _feat_cls.get_classes()\n",
        "    _nb_classes = len(_unique_classes)\n",
        "    _azi_list, _ele_list = _feat_cls.get_azi_ele_list()\n",
        "    _max_frames = _feat_cls.get_nb_frames()\n",
        "    _labels = np.zeros((_max_frames, _nb_classes, len(_azi_list) * len(_ele_list)))\n",
        "\n",
        "    for _frame_cnt in _output_dict.keys():\n",
        "        if _frame_cnt < _max_frames:\n",
        "            for _tmp_doa in _output_dict[_frame_cnt]:\n",
        "                \n",
        "                _tmp_doa[1] = np.clip(_tmp_doa[1], _azi_list[0], _azi_list[-1])\n",
        "                _tmp_doa[2] = np.clip(_tmp_doa[2], _ele_list[0], _ele_list[-1])\n",
        "\n",
        "                \n",
        "                _labels[_frame_cnt, _tmp_doa[0], int(_feat_cls.get_list_index(_tmp_doa[1], _tmp_doa[2]))] = 1\n",
        "\n",
        "    return _labels\n",
        "\n",
        "\n",
        "def regression_label_format_to_output_format(_feat_cls, _sed_labels, _doa_labels_deg):\n",
        "\n",
        "    _unique_classes = _feat_cls.get_classes()\n",
        "    _nb_classes = len(_unique_classes)\n",
        "    _azi_labels = _doa_labels_deg[:, :_nb_classes]\n",
        "    _ele_labels = _doa_labels_deg[:, _nb_classes:]\n",
        "\n",
        "    _output_dict = {}\n",
        "    for _frame_ind in range(_sed_labels.shape[0]):\n",
        "        _tmp_ind = np.where(_sed_labels[_frame_ind, :])\n",
        "        if len(_tmp_ind[0]):\n",
        "            _output_dict[_frame_ind] = []\n",
        "            for _tmp_class in _tmp_ind[0]:\n",
        "                _output_dict[_frame_ind].append([_tmp_class, _azi_labels[_frame_ind, _tmp_class], _ele_labels[_frame_ind, _tmp_class]])\n",
        "    return _output_dict\n",
        "\n",
        "\n",
        "def classification_label_format_to_output_format(_feat_cls, _labels):\n",
        "    _output_dict = {}\n",
        "    for _frame_ind in range(_labels.shape[0]):\n",
        "        _tmp_class_ind = np.where(_labels[_frame_ind].sum(1))\n",
        "        if len(_tmp_class_ind[0]):\n",
        "            _output_dict[_frame_ind] = []\n",
        "            for _tmp_class in _tmp_class_ind[0]:\n",
        "                _tmp_spatial_ind = np.where(_labels[_frame_ind, _tmp_class])\n",
        "                for _tmp_spatial in _tmp_spatial_ind[0]:\n",
        "                    _azi, _ele = _feat_cls.get_matrix_index(_tmp_spatial)\n",
        "                    _output_dict[_frame_ind].append(\n",
        "                        [_tmp_class, _azi, _ele])\n",
        "\n",
        "    return _output_dict\n",
        "\n",
        "\n",
        "def description_file_to_output_format(_desc_file_dict, _unique_classes, _hop_length_sec):\n",
        "\n",
        "    _output_dict = {}\n",
        "    for _ind, _tmp_start_sec in enumerate(_desc_file_dict['start']):\n",
        "        _tmp_class = _unique_classes[_desc_file_dict['class'][_ind]]\n",
        "        _tmp_azi = _desc_file_dict['azi'][_ind]\n",
        "        _tmp_ele = _desc_file_dict['ele'][_ind]\n",
        "        _tmp_end_sec = _desc_file_dict['end'][_ind]\n",
        "\n",
        "        _start_frame = int(_tmp_start_sec / _hop_length_sec)\n",
        "        _end_frame = int(_tmp_end_sec / _hop_length_sec)\n",
        "        for _frame_ind in range(_start_frame, _end_frame + 1):\n",
        "            if _frame_ind not in _output_dict:\n",
        "                _output_dict[_frame_ind] = []\n",
        "            _output_dict[_frame_ind].append([_tmp_class, _tmp_azi, _tmp_ele])\n",
        "\n",
        "    return _output_dict\n",
        "\n",
        "\n",
        "def load_output_format_file(_output_format_file):\n",
        "    _output_dict = {}\n",
        "    _fid = open(_output_format_file, 'r')\n",
        "    # next(_fid)\n",
        "    for _line in _fid:\n",
        "        _words = _line.strip().split(',')\n",
        "        _frame_ind = int(_words[0])\n",
        "        if _frame_ind not in _output_dict:\n",
        "            _output_dict[_frame_ind] = []\n",
        "        _output_dict[_frame_ind].append([int(_words[1]), int(_words[2]), int(_words[3])])\n",
        "    _fid.close()\n",
        "    return _output_dict\n",
        "\n",
        "\n",
        "def write_output_format_file(_output_format_file, _output_format_dict):\n",
        "    _fid = open(_output_format_file, 'w')\n",
        "    \n",
        "    for _frame_ind in _output_format_dict.keys():\n",
        "        for _value in _output_format_dict[_frame_ind]:\n",
        "            _fid.write('{},{},{},{}\\n'.format(int(_frame_ind), int(_value[0]), int(_value[1]), int(_value[2])))\n",
        "    _fid.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbKOxjzUX93e"
      },
      "source": [
        "# SELD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-Edue-bX_fh",
        "tags": []
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plot\n",
        "from keras.models import load_model\n",
        "import time\n",
        "cls_data_generator = DataGenerator\n",
        "cls_feature_class = FeatureClass\n",
        "\n",
        "plot.switch_backend('agg')\n",
        "\n",
        "\n",
        "def collect_test_labels(_data_gen_test, _data_out, quick_test):\n",
        "    # Collecting ground truth for test data\n",
        "    nb_batch = 2 if quick_test else _data_gen_test.get_total_batches_in_data()\n",
        "\n",
        "    batch_size = _data_out[0][0]\n",
        "    gt_sed = np.zeros((nb_batch * batch_size, _data_out[0][1], _data_out[0][2]))\n",
        "    gt_doa = np.zeros((nb_batch * batch_size, _data_out[0][1], _data_out[1][2]))\n",
        "\n",
        "    print(\"nb_batch in test: {}\".format(nb_batch))\n",
        "    cnt = 0\n",
        "    for tmp_feat, tmp_label in _data_gen_test.generate():\n",
        "        gt_sed[cnt * batch_size:(cnt + 1) * batch_size, :, :] = tmp_label[0]\n",
        "        gt_doa[cnt * batch_size:(cnt + 1) * batch_size, :, :] = tmp_label[1]\n",
        "        cnt = cnt + 1\n",
        "        if cnt == nb_batch:\n",
        "            break\n",
        "    return gt_sed.astype(int), gt_doa\n",
        "\n",
        "\n",
        "def plot_functions(fig_name, _tr_loss, _val_loss, _sed_loss, _doa_loss, _epoch_metric_loss):\n",
        "    plot.figure()\n",
        "    nb_epoch = len(_tr_loss)\n",
        "    plot.subplot(311)\n",
        "    plot.plot(range(nb_epoch), _tr_loss, label='train loss')\n",
        "    plot.plot(range(nb_epoch), _val_loss, label='val loss')\n",
        "    plot.legend()\n",
        "    plot.grid(True)\n",
        "\n",
        "    plot.subplot(312)\n",
        "    plot.plot(range(nb_epoch), _sed_loss[:, 0], label='sed er')\n",
        "    plot.plot(range(nb_epoch), _sed_loss[:, 1], label='sed f1')\n",
        "    plot.plot(range(nb_epoch), _doa_loss[:, 0]/180., label='doa er / 180')\n",
        "    plot.plot(range(nb_epoch), _doa_loss[:, 1], label='doa fr')\n",
        "    plot.plot(range(nb_epoch), _epoch_metric_loss, label='seld')\n",
        "    plot.legend()\n",
        "    plot.grid(True)\n",
        "\n",
        "    plot.subplot(313)\n",
        "    plot.plot(range(nb_epoch), _doa_loss[:, 2], label='pred_pks')\n",
        "    plot.plot(range(nb_epoch), _doa_loss[:, 3], label='good_pks')\n",
        "    plot.legend()\n",
        "    plot.grid(True)\n",
        "\n",
        "    plot.savefig(fig_name)\n",
        "    plot.close()\n",
        "\n",
        "\n",
        "def main(argv):\n",
        "    \n",
        "    if len(argv) != 3:\n",
        "        print('\\n\\n')\n",
        "        print('-------------------------------------------------------------------------------------------------------')\n",
        "        print('The code expected two optional inputs')\n",
        "        print('\\t>> python seld.py <task-id> <job-id>')\n",
        "        print('\\t\\t<task-id> is used to choose the user-defined parameter set from parameter.py')\n",
        "        print('Using default inputs for now')\n",
        "        print('\\t\\t<job-id> is a unique identifier which is used for output filenames (models, training plots). '\n",
        "              'You can use any number or string for this.')\n",
        "        print('-------------------------------------------------------------------------------------------------------')\n",
        "        print('\\n\\n')\n",
        "\n",
        "    # use parameter set defined by user\n",
        "    task_id = '1' if len(argv) < 2 else argv[1]\n",
        "    params = get_params(task_id)\n",
        "\n",
        "    job_id = 1 if len(argv) < 3 else argv[-1]\n",
        "\n",
        "    train_splits, val_splits, test_splits = None, None, None\n",
        "    if params['mode'] == 'dev':\n",
        "        test_splits = [3]\n",
        "        val_splits = [2]\n",
        "        train_splits = [[1]]\n",
        "\n",
        "\n",
        "    elif params['mode'] == 'eval':\n",
        "        test_splits = [0]\n",
        "        val_splits = [1]\n",
        "        train_splits = [[2, 3, 4]]\n",
        "\n",
        "    avg_scores_val = []\n",
        "    avg_scores_test = []\n",
        "    for split_cnt, split in enumerate(test_splits):\n",
        "        print('\\n\\n---------------------------------------------------------------------------------------------------')\n",
        "        print('------------------------------------      SPLIT {}   -----------------------------------------------'.format(split))\n",
        "        print('---------------------------------------------------------------------------------------------------')\n",
        "\n",
        "        # Unique name for the run\n",
        "        create_folder(params['model_dir'])\n",
        "        unique_name = '{}_{}_{}_{}_split{}'.format(\n",
        "            task_id, job_id, params['dataset'], params['mode'], split\n",
        "        )\n",
        "        unique_name = os.path.join(params['model_dir'], unique_name)\n",
        "        model_name = '{}_model.h5'.format(unique_name)\n",
        "        print(\"unique_name: {}\\n\".format(unique_name))\n",
        "\n",
        "        # Load train and validation data\n",
        "        print('Loading training dataset:')\n",
        "        data_gen_train = DataGenerator(\n",
        "            dataset=params['dataset'], split=train_splits[split_cnt], batch_size=params['batch_size'],\n",
        "            seq_len=params['sequence_length'], feat_label_dir=params['feat_label_dir']\n",
        "        )\n",
        "\n",
        "\n",
        "        print('Loading validation dataset:')\n",
        "        data_gen_val = DataGenerator(\n",
        "            dataset=params['dataset'], split=val_splits[split_cnt], batch_size=params['batch_size'],\n",
        "            seq_len=params['sequence_length'], feat_label_dir=params['feat_label_dir'], shuffle=False\n",
        "        )\n",
        "\n",
        "        \n",
        "        data_in, data_out = data_gen_train.get_data_sizes()\n",
        "        print('FEATURES:\\n\\tdata_in: {}\\n\\tdata_out: {}\\n'.format(data_in, data_out))\n",
        "\n",
        "        gt = collect_test_labels(data_gen_val, data_out, params['quick_test'])\n",
        "        sed_gt = reshape_3Dto2D(gt[0])\n",
        "        doa_gt = reshape_3Dto2D(gt[1])\n",
        "    \n",
        "\n",
        "        \n",
        "        nb_classes = data_gen_train.get_nb_classes()\n",
        "        def_elevation = data_gen_train.get_default_elevation()\n",
        "        doa_gt[:, nb_classes:] = doa_gt[:, nb_classes:] / (180. / def_elevation)\n",
        "        \n",
        "\n",
        "        print('MODEL:\\n\\tdropout_rate: {}\\n\\tCNN: nb_cnn_filt: {}, pool_size{}\\n\\trnn_size: {}, fnn_size: {}\\n'.format(\n",
        "            params['dropout_rate'], params['nb_cnn2d_filt'], params['pool_size'], params['rnn_size'],\n",
        "            params['fnn_size']))\n",
        "        \n",
        "\n",
        "        model = get_model(data_in=data_in, data_out=data_out, dropout_rate=params['dropout_rate'],\n",
        "                                      nb_cnn2d_filt=params['nb_cnn2d_filt'], pool_size=params['pool_size'],\n",
        "                                      rnn_size=params['rnn_size'], fnn_size=params['fnn_size'],\n",
        "                                      weights=params['loss_weights'])\n",
        "       \n",
        "        best_seld_metric = 99999\n",
        "        best_epoch = -1\n",
        "        patience_cnt = 0\n",
        "        seld_metric = np.zeros(params['nb_epochs'])\n",
        "        tr_loss = np.zeros(params['nb_epochs'])\n",
        "        val_loss = np.zeros(params['nb_epochs'])\n",
        "        doa_metric = np.zeros((params['nb_epochs'], 6))\n",
        "        sed_metric = np.zeros((params['nb_epochs'], 2))\n",
        "        nb_epoch = 2 if params['quick_test'] else params['nb_epochs']\n",
        "        \n",
        "\n",
        "        # start training\n",
        "        for epoch_cnt in range(nb_epoch):\n",
        "            start = time.time()\n",
        "\n",
        "            # train once per epoch\n",
        "            hist = model.fit_generator(\n",
        "                generator=data_gen_train.generate(),\n",
        "                steps_per_epoch=2 if params['quick_test'] else data_gen_train.get_total_batches_in_data(),\n",
        "                validation_data=data_gen_val.generate(),\n",
        "                validation_steps=2 if params['quick_test'] else data_gen_val.get_total_batches_in_data(),\n",
        "                epochs=params['epochs_per_fit'],\n",
        "                verbose=2\n",
        "            )\n",
        "            tr_loss[epoch_cnt] = hist.history.get('loss')[-1]\n",
        "            val_loss[epoch_cnt] = hist.history.get('val_loss')[-1]\n",
        "\n",
        "            # predict once per epoch\n",
        "            pred = model.predict_generator(\n",
        "                generator=data_gen_val.generate(),\n",
        "                steps=2 if params['quick_test'] else data_gen_val.get_total_batches_in_data(),\n",
        "                verbose=2\n",
        "            )\n",
        "\n",
        "            # Calculate the metrics\n",
        "            sed_pred = reshape_3Dto2D(pred[0]) > 0.5\n",
        "            doa_pred = reshape_3Dto2D(pred[1])\n",
        "\n",
        "            # rescaling the elevation data from [-180 180] to [-def_elevation def_elevation] for scoring purpose\n",
        "            doa_pred[:, nb_classes:] = doa_pred[:, nb_classes:] / (180. / def_elevation)\n",
        "\n",
        "            sed_metric[epoch_cnt, :] = evaluation_metrics.compute_sed_scores(sed_pred, sed_gt, data_gen_val.nb_frames_1s())\n",
        "            doa_metric[epoch_cnt, :] = evaluation_metrics.compute_doa_scores_regr(doa_pred, doa_gt, sed_pred, sed_gt)\n",
        "            seld_metric[epoch_cnt] = evaluation_metrics.compute_seld_metric(sed_metric[epoch_cnt, :], doa_metric[epoch_cnt, :])\n",
        "\n",
        "            # Visualize the metrics with respect to epochs\n",
        "            plot_functions(unique_name, tr_loss, val_loss, sed_metric, doa_metric, seld_metric)\n",
        "\n",
        "            patience_cnt += 1\n",
        "            if seld_metric[epoch_cnt] < best_seld_metric:\n",
        "                best_seld_metric = seld_metric[epoch_cnt]\n",
        "                best_epoch = epoch_cnt\n",
        "                model.save(model_name)\n",
        "                patience_cnt = 0\n",
        "\n",
        "            print(\n",
        "                'epoch_cnt: %d, time: %.2fs, tr_loss: %.2f, val_loss: %.2f, '\n",
        "                'ER_overall: %.2f, F1_overall: %.2f, '\n",
        "                'doa_error_pred: %.2f, good_pks_ratio:%.2f, '\n",
        "                'seld_score: %.2f, best_seld_score: %.2f, best_epoch : %d\\n' %\n",
        "                (\n",
        "                    epoch_cnt, time.time() - start, tr_loss[epoch_cnt], val_loss[epoch_cnt],\n",
        "                    sed_metric[epoch_cnt, 0], sed_metric[epoch_cnt, 1],\n",
        "                    doa_metric[epoch_cnt, 0], doa_metric[epoch_cnt, 1],\n",
        "                    seld_metric[epoch_cnt], best_seld_metric, best_epoch\n",
        "                )\n",
        "            )\n",
        "            if patience_cnt > params['patience']:\n",
        "                break\n",
        "\n",
        "        avg_scores_val.append([sed_metric[best_epoch, 0], sed_metric[best_epoch, 1], doa_metric[best_epoch, 0],\n",
        "                               doa_metric[best_epoch, 1], best_seld_metric])\n",
        "        print('\\nResults on validation split:')\n",
        "        print('\\tUnique_name: {} '.format(unique_name))\n",
        "        print('\\tSaved model for the best_epoch: {}'.format(best_epoch))\n",
        "        print('\\tSELD_score: {}'.format(best_seld_metric))\n",
        "        print('\\tDOA Metrics: DOA_error: {}, frame_recall: {}'.format(doa_metric[best_epoch, 0],\n",
        "                                                                      doa_metric[best_epoch, 1]))\n",
        "        print('\\tSED Metrics: ER_overall: {}, F1_overall: {}\\n'.format(sed_metric[best_epoch, 0],\n",
        "                                                                       sed_metric[best_epoch, 1]))\n",
        "\n",
        "        # ------------------  Calculate metric scores for unseen test split ---------------------------------\n",
        "        print('Loading testing dataset:')\n",
        "        data_gen_test = DataGenerator(\n",
        "            dataset=params['dataset'], split=split, batch_size=params['batch_size'], seq_len=params['sequence_length'],\n",
        "            feat_label_dir=params['feat_label_dir'], shuffle=False, per_file=params['dcase_output'],\n",
        "            is_eval=True if params['mode'] is 'eval' else False\n",
        "        )\n",
        "\n",
        "        print('\\nLoading the best model and predicting results on the testing split')\n",
        "        model = load_model('{}_model.h5'.format(unique_name))\n",
        "        pred_test = model.predict_generator(\n",
        "            generator=data_gen_test.generate(),\n",
        "            steps=2 if params['quick_test'] else data_gen_test.get_total_batches_in_data(),\n",
        "            verbose=2\n",
        "        )\n",
        "\n",
        "        test_sed_pred = reshape_3Dto2D(pred_test[0]) > 0.5\n",
        "        test_doa_pred = reshape_3Dto2D(pred_test[1])\n",
        "\n",
        "        \n",
        "        test_doa_pred[:, nb_classes:] = test_doa_pred[:, nb_classes:] / (180. / def_elevation)\n",
        "\n",
        "        if params['dcase_output']:\n",
        "            \n",
        "            dcase_dump_folder = os.path.join(params['dcase_dir'], '{}_{}_{}'.format(task_id, params['dataset'], params['mode']))\n",
        "            create_folder(dcase_dump_folder)\n",
        "            print('Dumping recording-wise results in: {}'.format(dcase_dump_folder))\n",
        "\n",
        "            test_filelist = data_gen_test.get_filelist()\n",
        "            \n",
        "            max_frames_with_content = data_gen_test.get_nb_frames()\n",
        "\n",
        "           \n",
        "            frames_per_file = data_gen_test.get_frame_per_file()\n",
        "\n",
        "            for file_cnt in range(test_sed_pred.shape[0]//frames_per_file):\n",
        "                output_file = os.path.join(dcase_dump_folder, test_filelist[file_cnt].replace('.npy', '.csv'))\n",
        "                dc = file_cnt * frames_per_file\n",
        "                output_dict = evaluation_metrics.regression_label_format_to_output_format(\n",
        "                    data_gen_test,\n",
        "                    test_sed_pred[dc:dc + max_frames_with_content, :],\n",
        "                    test_doa_pred[dc:dc + max_frames_with_content, :] * 180 / np.pi\n",
        "                )\n",
        "                evaluation_metrics.write_output_format_file(output_file, output_dict)\n",
        "\n",
        "        if params['mode'] is 'dev':\n",
        "            test_data_in, test_data_out = data_gen_test.get_data_sizes()\n",
        "            test_gt = collect_test_labels(data_gen_test, test_data_out, params['quick_test'])\n",
        "            test_sed_gt = reshape_3Dto2D(test_gt[0])\n",
        "            test_doa_gt = reshape_3Dto2D(test_gt[1])\n",
        "            \n",
        "            test_doa_gt[:, nb_classes:] = test_doa_gt[:, nb_classes:] / (180. / def_elevation)\n",
        "\n",
        "            test_sed_loss = evaluation_metrics.compute_sed_scores(test_sed_pred, test_sed_gt, data_gen_test.nb_frames_1s())\n",
        "            test_doa_loss = evaluation_metrics.compute_doa_scores_regr(test_doa_pred, test_doa_gt, test_sed_pred, test_sed_gt)\n",
        "            test_metric_loss = evaluation_metrics.compute_seld_metric(test_sed_loss, test_doa_loss)\n",
        "\n",
        "            avg_scores_test.append([test_sed_loss[0], test_sed_loss[1], test_doa_loss[0], test_doa_loss[1], test_metric_loss])\n",
        "            print('Results on test split:')\n",
        "            print('\\tSELD_score: {},  '.format(test_metric_loss))\n",
        "            print('\\tDOA Metrics: DOA_error: {}, frame_recall: {}'.format(test_doa_loss[0], test_doa_loss[1]))\n",
        "            print('\\tSED Metrics: ER_overall: {}, F1_overall: {}\\n'.format(test_sed_loss[0], test_sed_loss[1]))\n",
        "\n",
        "    print('\\n\\nValidation split scores per fold:\\n')\n",
        "    for cnt in range(len(val_splits)):\n",
        "        print('\\tSplit {} - SED ER: {} F1: {}; DOA error: {} frame recall: {}; SELD score: {}'.format(cnt, avg_scores_val[cnt][0], avg_scores_val[cnt][1], avg_scores_val[cnt][2], avg_scores_val[cnt][3], avg_scores_val[cnt][4]))\n",
        "\n",
        "    if params['mode'] is 'dev':\n",
        "        print('\\n\\nTesting split scores per fold:\\n')\n",
        "        for cnt in range(len(val_splits)):\n",
        "            print('\\tSplit {} - SED ER: {} F1: {}; DOA error: {} frame recall: {}; SELD score: {}'.format(cnt, avg_scores_test[cnt][0], avg_scores_test[cnt][1], avg_scores_test[cnt][2], avg_scores_test[cnt][3], avg_scores_test[cnt][4]))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        sys.exit(main(sys.argv))\n",
        "    except (ValueError, IOError) as e:\n",
        "        sys.exit(e)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwZB_6PkO8R0"
      },
      "source": [
        "# Visualizing Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JogtTHVO8R1"
      },
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import PyQt5\n",
        "import numpy as np\n",
        "import librosa.display\n",
        "import sys\n",
        "sys.path.append(os.path.join(sys.path[0], '..'))\n",
        "from metrics import evaluation_metrics\n",
        "import cls_feature_class\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.pyplot as plot\n",
        "plot.switch_backend('Qt4Agg')\n",
        "\n",
        "\n",
        "\n",
        "def collect_classwise_data(_in_dict):\n",
        "    _out_dict = {}\n",
        "    for _key in _in_dict.keys():\n",
        "        for _seld in _in_dict[_key]:\n",
        "            if _seld[0] not in _out_dict:\n",
        "                _out_dict[_seld[0]] = []\n",
        "            _out_dict[_seld[0]].append([_key, _seld[0], _seld[1], _seld[2]])\n",
        "    return _out_dict\n",
        "\n",
        "\n",
        "def plot_func(plot_data, hop_len_s, ind, plot_x_ax=False):\n",
        "    cmap = ['b', 'r', 'g', 'y', 'k', 'c', 'm', 'b', 'r', 'g', 'y', 'k', 'c', 'm']\n",
        "    for class_ind in plot_data.keys():\n",
        "        time_ax = np.array(plot_data[class_ind])[:, 0] *hop_len_s\n",
        "        y_ax = np.array(plot_data[class_ind])[:, ind]\n",
        "        plot.plot(time_ax, y_ax, marker='.', color=cmap[class_ind], linestyle='None', markersize=4)\n",
        "    plot.grid()\n",
        "    plot.xlim([0, 60])\n",
        "    if not plot_x_ax:\n",
        "        plot.tick_params(\n",
        "            axis='x',  \n",
        "            which='both',  \n",
        "            bottom='off',  \n",
        "            top='off',  \n",
        "            labelbottom='off')  \n",
        "\n",
        "\n",
        "# --------------------------------- MAIN SCRIPT STARTS HERE -----------------------------------------\n",
        "\n",
        "hop_s = 0.02\n",
        "\n",
        "pred = 'D:\\sap\\seld-dcase2019-master\\dcase\\mic_dev\\split1_ir0_ov1_2.csv'\n",
        "\n",
        "\n",
        "ref_dir = 'D:\\sap\\metadata_dev'\n",
        "aud_dir = 'D:\\sap\\mic_dev'\n",
        "\n",
        "pred_dict = evaluation_metrics.load_output_format_file(pred)\n",
        "\n",
        "feat_cls = cls_feature_class.FeatureClass()\n",
        "ref_filename = os.path.basename(pred)\n",
        "ref_desc_dict = feat_cls.read_desc_file(os.path.join(ref_dir, ref_filename), in_sec=True)\n",
        "ref_dict = evaluation_metrics.description_file_to_output_format(ref_desc_dict, feat_cls.get_classes(), hop_s)\n",
        "\n",
        "\n",
        "pred_data = collect_classwise_data(pred_dict)\n",
        "ref_data = collect_classwise_data(ref_dict)\n",
        "\n",
        "nb_classes = len(feat_cls.get_classes())\n",
        "\n",
        "ref_filename = os.path.basename(pred).replace('.csv', '.wav')\n",
        "audio, fs = feat_cls._load_audio(os.path.join(aud_dir, ref_filename))\n",
        "stft = np.abs(np.squeeze(feat_cls._spectrogram(audio[:, :1])))\n",
        "stft = librosa.amplitude_to_db(stft, ref=np.max)\n",
        "\n",
        "plot.figure()\n",
        "gs = gridspec.GridSpec(4, 4)\n",
        "ax0 = plot.subplot(gs[0, 1:3]), librosa.display.specshow(stft.T, sr=fs, x_axis='time', y_axis='linear'), plot.title('Spectrogram')\n",
        "ax1 = plot.subplot(gs[1, :2]), plot_func(ref_data, hop_s, ind=1), plot.ylim([-1, nb_classes + 1]), plot.title('SED reference')\n",
        "ax2 = plot.subplot(gs[1, 2:]), plot_func(pred_data, hop_s, ind=1), plot.ylim([-1, nb_classes + 1]), plot.title('SED predicted')\n",
        "ax3 = plot.subplot(gs[2, :2]), plot_func(ref_data, hop_s, ind=2), plot.ylim([-190, 190]), plot.title('Azimuth DOA reference')\n",
        "ax4 = plot.subplot(gs[2, 2:]), plot_func(pred_data, hop_s, ind=2), plot.ylim([-190, 190]), plot.title('Azimuth DOA predicted')\n",
        "ax5 = plot.subplot(gs[3, :2]), plot_func(ref_data, hop_s, ind=3, plot_x_ax=True), plot.ylim([-50, 50]), plot.title('Elevation DOA reference')\n",
        "ax6 = plot.subplot(gs[3, 2:]), plot_func(pred_data, hop_s, ind=3, plot_x_ax=True), plot.ylim([-50, 50]), plot.title('Elevation DOA predicted')\n",
        "ax_lst = [ax0, ax1, ax2, ax3, ax4, ax5, ax6]\n",
        "plot.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4y5mywgPLun"
      },
      "source": [
        "#SELD Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN0EvcZQPNNy"
      },
      "source": [
        "import os\n",
        "from metrics import evaluation_metrics\n",
        "import cls_feature_class\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_nb_files(_pred_file_list, _group='split'):\n",
        "    _group_ind = {'split': 5, 'ir': 9, 'ov': 13}\n",
        "    _cnt_dict = {}\n",
        "    for _filename in _pred_file_list:\n",
        "\n",
        "        if _group == 'all':\n",
        "            _ind = 0\n",
        "        else:\n",
        "            _ind = int(_filename[_group_ind[_group]])\n",
        "\n",
        "        if _ind not in _cnt_dict:\n",
        "            _cnt_dict[_ind] = []\n",
        "        _cnt_dict[_ind].append(_filename)\n",
        "\n",
        "    return _cnt_dict\n",
        "\n",
        "\n",
        "\n",
        "ref_desc_files = 'D:\\sap\\metadata_dev' \n",
        "pred_output_format_files = 'D:\\sap\\results\\2_mic_dev' \n",
        "\n",
        "feat_cls = cls_feature_class.FeatureClass()\n",
        "max_frames = feat_cls.get_nb_frames()\n",
        "unique_classes = feat_cls.get_classes()\n",
        "nb_classes = len(unique_classes)\n",
        "azi_list, ele_list = feat_cls.get_azi_ele_list()\n",
        "\n",
        "ref_files = os.listdir(ref_desc_files)\n",
        "nb_ref_files = len(ref_files)\n",
        "\n",
        "pred_files = os.listdir(pred_output_format_files)\n",
        "nb_pred_files = len(pred_files)\n",
        "\n",
        "if nb_ref_files != nb_pred_files:\n",
        "    print('ERROR: Mismatch. Reference has {} and prediction has {} files'.format(nb_ref_files, nb_pred_files))\n",
        "    exit()\n",
        "\n",
        "eval = evaluation_metrics.SELDMetrics(nb_frames_1s=feat_cls.nb_frames_1s(), data_gen=feat_cls)\n",
        "\n",
        "\n",
        "score_type_list = [ 'all', 'split', 'ov', 'ir']\n",
        "\n",
        "print('\\nCalculating {} scores for {}'.format(score_type_list, os.path.basename(pred_output_format_files)))\n",
        "\n",
        "for score_type in score_type_list:\n",
        "    print('\\n\\n---------------------------------------------------------------------------------------------------')\n",
        "    print('------------------------------------  {}   ---------------------------------------------'.format('Total score' if score_type=='all' else 'score per {}'.format(score_type)))\n",
        "    print('---------------------------------------------------------------------------------------------------')\n",
        "\n",
        "    split_cnt_dict = get_nb_files(pred_files, _group=score_type) \n",
        "\n",
        "    \n",
        "    for split_key in np.sort(list(split_cnt_dict)):\n",
        "        eval.reset()    \n",
        "        for pred_cnt, pred_file in enumerate(split_cnt_dict[split_key]):\n",
        "            \n",
        "            pred_dict = evaluation_metrics.load_output_format_file(os.path.join(pred_output_format_files, pred_file))\n",
        "\n",
        "            \n",
        "            gt_desc_file_dict = feat_cls.read_desc_file(os.path.join(ref_desc_files, pred_file.replace('.npy', '.csv')))\n",
        "\n",
        "            \n",
        "            gt_labels = feat_cls.get_clas_labels_for_file(gt_desc_file_dict)\n",
        "            pred_labels = evaluation_metrics.output_format_dict_to_classification_labels(pred_dict, feat_cls)\n",
        "\n",
        "            \n",
        "            eval.update_sed_scores(pred_labels.max(2), gt_labels.max(2))\n",
        "            eval.update_doa_scores(pred_labels, gt_labels)\n",
        "\n",
        "        \n",
        "        er, f = eval.compute_sed_scores()\n",
        "        doa_err, frame_recall = eval.compute_doa_scores()\n",
        "        seld_scr = evaluation_metrics.compute_seld_metric([er, f], [doa_err, frame_recall])\n",
        "\n",
        "        print('\\nAverage score for {} {} data'.format(score_type, 'fold' if score_type=='all' else split_key))\n",
        "        print('SELD score: {}'.format(seld_scr))\n",
        "        print('SED metrics: er: {}, f:{}'.format(er, f))\n",
        "        print('DOA metrics: doa error: {}, frame recall:{}'.format(doa_err, frame_recall))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}